{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import azure.batch.models as batchmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERS TO KEEP!\n",
    "\n",
    "def find_files(directory, extension):\n",
    "    \"\"\"Lists files of a specified extension in the target directory.\n",
    "\n",
    "    :param str directory: The directory to explore.\n",
    "    :param str extension: The file extension to search for.\n",
    "    :return str: List of files with extension found\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(extension):\n",
    "            files.append(os.path.abspath(os.path.join(directory, file)))\n",
    "    return sorted(files)\n",
    "\n",
    "def get_case_files(case_directory):\n",
    "    \"\"\"Gets the Radiance case files to be uploaded and processed.\n",
    "\n",
    "    :param str case_directory: The case directory from which files will be grabbed.\n",
    "    :return (str, str, [str]): Sky matrix, Surfaces, Analysis Grid/s\n",
    "    \"\"\"\n",
    "    sky_matrix = os.path.join(case_directory, \"sky_mtx.json\")\n",
    "    surfaces = os.path.join(case_directory, \"surfaces.json\")\n",
    "    grids = find_files(os.path.join(case_directory, \"AnalysisGrids\"), \".json\")\n",
    "    \n",
    "    return sky_matrix, surfaces, grids\n",
    "\n",
    "def wrap_commands_in_shell(ostype, commands):\n",
    "    \"\"\"\n",
    "    Wrap commands in a shell\n",
    "\n",
    "    :param list commands: list of commands to wrap\n",
    "    :param str ostype: OS type, linux or windows\n",
    "    :rtype: str\n",
    "    :return: a shell wrapping commands\n",
    "    \"\"\"\n",
    "    if ostype.lower() == \"linux\":\n",
    "        return \"/bin/bash -c \\\"set -e; set -o pipefail; {0:}; wait\\\"\".format(\";\".join(commands))\n",
    "    elif ostype.lower() == \"windows\":\n",
    "        return \"cmd.exe /c {0:}\".format(\"&\".join(commands))\n",
    "    else:\n",
    "        raise ValueError(\"unknown ostype: {}\".format(ostype))\n",
    "\n",
    "\n",
    "def select_latest_verified_vm_image_with_node_agent_sku(\n",
    "        batch_client, publisher, offer, sku_starts_with):\n",
    "    \"\"\"Select the latest verified image that Azure Batch supports given\n",
    "    a publisher, offer and sku (starts with filter).\n",
    "\n",
    "    :param batch_client: The batch client to use.\n",
    "    :type batch_client: `batchserviceclient.BatchServiceClient`\n",
    "    :param str publisher: vm image publisher\n",
    "    :param str offer: vm image offer\n",
    "    :param str sku_starts_with: vm sku starts with filter\n",
    "    :rtype: tuple\n",
    "    :return: (node agent sku id to use, vm image ref to use)\n",
    "    \"\"\"\n",
    "    # get verified vm image list and node agent sku ids from service\n",
    "    node_agent_skus = batch_client.account.list_node_agent_skus()\n",
    "    # pick the latest supported sku\n",
    "    skus_to_use = [\n",
    "        (sku, image_ref) for sku in node_agent_skus for image_ref in sorted(\n",
    "            sku.verified_image_references, key=lambda item: item.sku)\n",
    "        if image_ref.publisher.lower() == publisher.lower() and\n",
    "        image_ref.offer.lower() == offer.lower() and\n",
    "        image_ref.sku.startswith(sku_starts_with)\n",
    "    ]\n",
    "    # skus are listed in reverse order, pick first for latest\n",
    "    sku_to_use, image_ref_to_use = skus_to_use[0]\n",
    "    return (sku_to_use.id, image_ref_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000000-testjob-3513] container created.\n",
      "[0000000-testjob-3513] < case\\surfaces.json\n",
      "[0000000-testjob-3513] < case\\sky_mtx.json\n",
      "[0000000-testjob-3513] < case\\AnalysisGrids\\zone1.json\n",
      "[0000000-testjob-3513] < case\\AnalysisGrids\\zone2.json\n",
      "[0000000-testjob-3513] < case\\AnalysisGrids\\zone3.json\n",
      "[0000000-testjob-3513] < case\\AnalysisGrids\\zone4.json\n",
      "Attempting to create pool: batchpool\n"
     ]
    },
    {
     "ename": "BatchErrorException",
     "evalue": "{'additional_properties': {}, 'lang': 'en-US', 'value': 'The value provided for one of the properties in the request body is invalid.\\nRequestId:2c3ddeb6-329b-447c-b605-6affb3de9e0f\\nTime:2018-10-26T14:46:15.6246340Z'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBatchErrorException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-55cfb7428529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Attempting to create pool:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[0mbatch_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Created pool:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mbatchmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchErrorException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\azurebatch\\lib\\site-packages\\azure\\batch\\operations\\pool_operations.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, pool, pool_add_options, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchErrorException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[0mclient_raw_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClientRawResponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             client_raw_response.add_headers({\n",
      "\u001b[1;31mBatchErrorException\u001b[0m: {'additional_properties': {}, 'lang': 'en-US', 'value': 'The value provided for one of the properties in the request body is invalid.\\nRequestId:2c3ddeb6-329b-447c-b605-6affb3de9e0f\\nTime:2018-10-26T14:46:15.6246340Z'}"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Send job to Azure\")\n",
    "    \n",
    "    parser.add_argument(\"-d\", \"--caseDirectory\", type=str,\n",
    "        help=\"Path to the case directory from which simulation ingredients are obtained\",\n",
    "        default=r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\case\")  # TODO - Remove post testing\n",
    "    parser.add_argument(\"-c\", \"--configFile\", type=str,\n",
    "        help=\"Path to the azure config file\",\n",
    "        default=\"./azure_configuration.cfg\")  # TODO - Remove post testing\n",
    "    parser.add_argument(\"-j\", \"--jobID\", type=str,\n",
    "        help=\"ID for the job being undertaken\",\n",
    "        default=\"0000000-testjob-3513\")  # TODO - Remove post testing\n",
    "    \n",
    "    # args = parser.parse_args()  # TODO - Uncomment for non-interactive\n",
    "    args = parser.parse_args(args=[])  # TODO - Remove for non-interactive\n",
    "    \n",
    "    _CASE_DIRECTORY = args.caseDirectory\n",
    "    _JOB_ID = args.jobID\n",
    "    _CONFIG_FILE = args.configFile\n",
    "    \n",
    "    global_config = configparser.RawConfigParser()\n",
    "    global_config.read(_CONFIG_FILE)\n",
    "    \n",
    "    _BATCH_ACCOUNT_NAME = global_config.get(\"Batch\", \"batchaccountname\")\n",
    "    _BATCH_ACCOUNT_KEY = global_config.get(\"Batch\", \"batchaccountkey\")\n",
    "    _BATCH_ACCOUNT_URL = global_config.get(\"Batch\", \"batchserviceurl\")\n",
    "    _STORAGE_ACCOUNT_NAME = global_config.get(\"Storage\", \"storageaccountname\")\n",
    "    _STORAGE_ACCOUNT_KEY = global_config.get(\"Storage\", \"storageaccountkey\")\n",
    "    _STORAGE_ACCOUNT_SUFFIX = global_config.get(\"Storage\", \"storageaccountsuffix\")\n",
    "    _POOL_ID = global_config.get(\"Default\", \"poolid\")\n",
    "    _POOL_VM_SIZE = global_config.get(\"Default\", \"poolvmsize\")\n",
    "    _MIN_POOL_NODE = global_config.getint(\"Default\", \"poolvmcountmin\")\n",
    "    _MAX_POOL_NODE = global_config.getint(\"Default\", \"poolvmcountmax\")\n",
    "    _NODE_OS_PUBLISHER = global_config.get(\"Default\", \"nodepublisher\")\n",
    "    _NODE_OS_OFFER = global_config.get(\"Default\", \"nodeoffer\")\n",
    "    _NODE_OS_SKU = global_config.get(\"Default\", \"nodesku\")\n",
    "    \n",
    "    _RADIANCE_SAS_TOKEN = global_config.get(\"Process\", \"radiancesastoken\")\n",
    "    _RADIANCE_SAS_URL = global_config.get(\"Process\", \"radiancesasurl\")\n",
    "    _LB_HB_SAS_TOKEN = global_config.get(\"Process\", \"lbhbsastoken\")\n",
    "    _LB_HB_SAS_URL = global_config.get(\"Process\", \"lbhbsasurl\")\n",
    "    _SCRIPT_SAS_TOKEN = global_config.get(\"Process\", \"scriptsastoken\")\n",
    "    _SCRIPT_SAS_URL = global_config.get(\"Process\", \"scriptsasurl\")\n",
    "    _COPYTOBLOB_SAS_TOKEN = global_config.get(\"Process\", \"copytoblobsastoken\")\n",
    "    _COPYTOBLOB_SAS_URL = global_config.get(\"Process\", \"copytoblobsasurl\")\n",
    "    \n",
    "    _DELETE_CONTAINER = global_config.getboolean(\"Default\", \"shoulddeletecontainer\")\n",
    "    _DELETE_JOB = global_config.getboolean(\"Default\", \"shoulddeletejob\")\n",
    "    _DELETE_POOL = global_config.getboolean(\"Default\", \"shoulddeletepool\")\n",
    "\n",
    "    # Create the blob client, for use in obtaining references to blob storage containers and uploading files to containers\n",
    "    blob_client = azureblob.BlockBlobService(account_name=_STORAGE_ACCOUNT_NAME, account_key=_STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "    # Create a job container\n",
    "    blob_client.create_container(_JOB_ID, fail_on_exist=False)\n",
    "    print('[{0:}] container created.'.format(_JOB_ID))\n",
    "\n",
    "##     # Create a common processing files container\n",
    "##     blob_client.create_container(\"0000000-common\", fail_on_exist=False)\n",
    "##     print('[{0:}] container created.'.format(\"0000000-common\"))\n",
    "##     print(\"\")\n",
    "\n",
    "##     # Upload the processing files to the blob\n",
    "##     radiance_file = upload_file_to_container(blob_client, \"0000000-common\", _RADIANCE)\n",
    "##     lb_hb_file = upload_file_to_container(blob_client, \"0000000-common\", _LB_HB)\n",
    "##     run_process_file = upload_file_to_container(blob_client, \"0000000-common\", _SCRIPT)\n",
    "    \n",
    "    # Upload case files to the blob, and obtain file sas urls\n",
    "    expiry = datetime.datetime.utcnow() + datetime.timedelta(minutes=global_config.getint(\"Default\", \"blobreadtimeout\"))\n",
    "    \n",
    "    _SKY_MTX_FILEPATH, _SURFACES_FILEPATH, _ANALYSIS_GRIDS_FILEPATHS = get_case_files(_CASE_DIRECTORY)\n",
    "    \n",
    "    surfaces_file = upload_file_to_container(blob_client, _JOB_ID, _SURFACES_FILEPATH)\n",
    "    surfaces_sas_token = blob_client.generate_blob_shared_access_signature(_JOB_ID, surfaces_file.file_path, permission=azureblob.BlobPermissions.READ, expiry=expiry)\n",
    "    surfaces_sas_url = blob_client.make_blob_url(_JOB_ID, surfaces_file.file_path, sas_token=surfaces_sas_token)\n",
    "\n",
    "    sky_mtx_file = upload_file_to_container(blob_client, _JOB_ID, _SKY_MTX_FILEPATH)\n",
    "    sky_mtx_sas_token = blob_client.generate_blob_shared_access_signature(_JOB_ID, sky_mtx_file.file_path, permission=azureblob.BlobPermissions.READ, expiry=expiry)\n",
    "    sky_mtx_sas_url = blob_client.make_blob_url(_JOB_ID, sky_mtx_file.file_path, sas_token=sky_mtx_sas_token)\n",
    "    \n",
    "    analysis_grid_files = [upload_file_to_container(blob_client, _JOB_ID, file_path) for file_path in _ANALYSIS_GRIDS_FILEPATHS]\n",
    "    analysis_grid_sas_tokens = []\n",
    "    analysis_grid_sas_urls = []\n",
    "    for i in analysis_grid_files:\n",
    "        analysis_grid_sas_token = blob_client.generate_blob_shared_access_signature(_JOB_ID, i.file_path, permission=azureblob.BlobPermissions.READ, expiry=expiry)\n",
    "        analysis_grid_sas_tokens.append(analysis_grid_sas_token)\n",
    "        analysis_grid_sas_urls.append(blob_client.make_blob_url(_JOB_ID, i.file_path, sas_token=analysis_grid_sas_token))\n",
    "\n",
    "    # Get a number of files to be processed\n",
    "    _POOL_NODE_COUNT = len(analysis_grid_files)\n",
    "    \n",
    "    # Generate batch credentials\n",
    "    batch_credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME, _BATCH_ACCOUNT_KEY)\n",
    "    \n",
    "    # Generate batch client for transaction handling\n",
    "    batch_client = batch.BatchServiceClient(batch_credentials, base_url=_BATCH_ACCOUNT_URL)\n",
    "\n",
    "    # Create a job\n",
    "    u_job_id = \"{0:}-{1:}\".format(_JOB_ID.replace(\"-\", \"\"), uuid.uuid1())\n",
    "    job = batchmodels.JobAddParameter(id=u_job_id, pool_info=batchmodels.PoolInformation(pool_id=_POOL_ID))\n",
    "\n",
    "    # Assign the job to the batch client\n",
    "    batch_client.job.add(job)\n",
    "    \n",
    "    # Add tasks to the job\n",
    "    tasks = []\n",
    "    for n, i in enumerate(analysis_grid_files):\n",
    "        task_id = i.file_path.replace(\".json\", \"\")\n",
    "        commands = wrap_commands_in_shell(\"linux\", [\n",
    "            \n",
    "            \"sudo cp -p {0:} $AZ_BATCH_NODE_SHARED_DIR\".format(i.file_path),\n",
    "            \n",
    "#             # Run the simulation\n",
    "#             \"sudo python RunHoneybeeRadiance.py -s surfaces.json -sm sky_mtx.json -p {0:}\".format(i.file_path),\n",
    "#             # Copy the results back to the blob\n",
    "#             \"sudo python copy_to_blob.py --filepath {0:} --blobname {1:} --storageaccount {2:} --storagecontainer {0:} --sastoken {3:}\".format(\n",
    "#                 i.file_path.replace(\".json\", \"_results.json\"),\n",
    "#                 _JOB_ID,\n",
    "#                 _STORAGE_ACCOUNT_NAME,\n",
    "#                 _STORAGE_ACCOUNT_KEY\n",
    "#             )\n",
    "        ])\n",
    "\n",
    "        resource_files=[i]\n",
    "\n",
    "    #     print(task_id, \"\\n\", commands, \"\\n\")\n",
    "        tasks.append(\n",
    "            batchmodels.TaskAddParameter(\n",
    "                id=task_id,\n",
    "                command_line=commands,\n",
    "                resource_files=resource_files\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for task in tasks:\n",
    "        batch_client.task.add(job_id=job.id, task=task)\n",
    "    \n",
    "    \n",
    "    # Get the latest VM image to run\n",
    "    sku_to_use, image_ref_to_use = select_latest_verified_vm_image_with_node_agent_sku(\n",
    "        batch_client, _NODE_OS_PUBLISHER, _NODE_OS_OFFER, _NODE_OS_SKU)\n",
    "\n",
    "    # Generate a pool\n",
    "    pool = batchmodels.PoolAddParameter(\n",
    "            id=_POOL_ID,\n",
    "            virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "                image_reference=image_ref_to_use,\n",
    "                node_agent_sku_id=sku_to_use),\n",
    "            vm_size=_POOL_VM_SIZE,\n",
    "            target_dedicated_nodes=_POOL_NODE_COUNT,\n",
    "            start_task=batchmodels.StartTask(\n",
    "                command_line=wrap_commands_in_shell(\"linux\", [\n",
    "                    \"cd\",\n",
    "                    \"cd /\",\n",
    "                    # Set up radiance software\n",
    "                    \"sudo cp -p radiance-5.1.0-Linux.tar.gz $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "                    \"sudo tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "                    \"sudo rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "                    \"sudo rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "                    # Set up Ladybug tools\n",
    "                    \"sudo cp -p lb_hb.tar.gz $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "                    \"sudo tar xzf lb_hb.tar.gz\",\n",
    "                    # Get the script and files to be run\n",
    "                    \"sudo cp -p RunHoneybeeRadiance.py $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "                    \"sudo cp -p copy_to_blob.py $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "                    \"sudo cp -p surfaces.json $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "                    \"sudo cp -p sky_mtx.json $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "                ]),\n",
    "                resource_files=[\n",
    "                    batchmodels.ResourceFile(file_path=\"radiance-5.1.0-Linux.tar.gz\", blob_source=_RADIANCE_SAS_URL),\n",
    "                    batchmodels.ResourceFile(file_path=\"lb_hb.tar.gz\", blob_source=_LB_HB_SAS_URL),\n",
    "                    batchmodels.ResourceFile(file_path=\"RunHoneybeeRadiance.py\", blob_source=_SCRIPT_SAS_URL),\n",
    "                    batchmodels.ResourceFile(file_path=\"copy_to_blob.py\", blob_source=_COPYTOBLOB_SAS_URL),\n",
    "                    batchmodels.ResourceFile(file_path=\"sky_mtx.json\", blob_source=sky_mtx_sas_url),\n",
    "                    batchmodels.ResourceFile(file_path=\"surfaces.json\", blob_source=surfaces_sas_url),\n",
    "                    surfaces,\n",
    "                    sky_mtx,\n",
    "                ]))\n",
    "\n",
    "    # Create the pool if it doesn't exist\n",
    "    try:\n",
    "        print(\"Attempting to create pool:\", pool.id)\n",
    "        batch_client.pool.add(pool)\n",
    "        print(\"Created pool:\", pool.id)\n",
    "    except batchmodels.BatchErrorException as e:\n",
    "        if e.error.code != \"PoolExists\":\n",
    "            raise\n",
    "        else:\n",
    "            print(\"Pool {!r} already exists\".format(pool.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create pool: batchpool\n",
      "Pool 'batchpool' already exists\n"
     ]
    }
   ],
   "source": [
    "task_commands = [\n",
    "    \"cd\",\n",
    "    \"cd /\",\n",
    "    # Set up radiance software\n",
    "    \"sudo cp -p radiance-5.1.0-Linux.tar.gz $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "    \"sudo tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "    \"sudo rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "    \"sudo rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "    # Set up Ladybug tools\n",
    "    \"sudo cp -p lb_hb.tar.gz $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "    \"sudo tar xzf lb_hb.tar.gz\",\n",
    "    # Get the script and files to be run\n",
    "    \"sudo cp -p RunHoneybeeRadiance.py $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "    \"sudo cp -p copy_to_blob.py $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "    \"sudo cp -p surfaces.json $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "    \"sudo cp -p sky_mtx.json $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_properties': {},\n",
       " 'id': 'zone1',\n",
       " 'display_name': None,\n",
       " 'command_line': '/bin/bash -c \"set -e; set -o pipefail; cd;cd /;sudo cp -p radiance-5.1.0-Linux.tar.gz $AZ_BATCH_NODE_SHARED_DIR;sudo tar xzf radiance-5.1.0-Linux.tar.gz;sudo rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/;sudo rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/;sudo cp -p lb_hb.tar.gz $AZ_BATCH_NODE_SHARED_DIR;sudo tar xzf lb_hb.tar.gz;sudo cp -p RunHoneybeeRadiance.py $AZ_BATCH_NODE_SHARED_DIR;sudo cp -p copy_to_blob.py $AZ_BATCH_NODE_SHARED_DIR;sudo cp -p zone1.json $AZ_BATCH_NODE_SHARED_DIR;sudo cp -p surfaces.json $AZ_BATCH_NODE_SHARED_DIR;sudo cp -p sky_mtx.json $AZ_BATCH_NODE_SHARED_DIR; wait\"',\n",
       " 'container_settings': None,\n",
       " 'exit_conditions': None,\n",
       " 'resource_files': [<azure.batch.models.resource_file.ResourceFile at 0x22f3911ccf8>,\n",
       "  <azure.batch.models.resource_file.ResourceFile at 0x22f38ea2e80>,\n",
       "  <azure.batch.models.resource_file.ResourceFile at 0x22f38ea2438>,\n",
       "  <azure.batch.models.resource_file.ResourceFile at 0x22f38ea2e48>,\n",
       "  <azure.batch.models.resource_file.ResourceFile at 0x22f3911be80>,\n",
       "  <azure.batch.models.resource_file.ResourceFile at 0x22f38f0e828>,\n",
       "  <azure.batch.models.resource_file.ResourceFile at 0x22f39103b70>],\n",
       " 'output_files': None,\n",
       " 'environment_settings': None,\n",
       " 'affinity_info': None,\n",
       " 'constraints': None,\n",
       " 'user_identity': None,\n",
       " 'multi_instance_settings': None,\n",
       " 'depends_on': None,\n",
       " 'application_package_references': None,\n",
       " 'authentication_token_settings': None}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FILES\n",
    "\n",
    "def print_configuration(config):\n",
    "    \"\"\"Prints the configuration being used as a dictionary\n",
    "\n",
    "    :param config: The configuration.\n",
    "    :type config: `configparser.ConfigParser`\n",
    "    \"\"\"\n",
    "    configuration_dict = {s: dict(config.items(s)) for s in\n",
    "                          config.sections() + ['DEFAULT']}\n",
    "\n",
    "    print(\"Configuration is:\")\n",
    "    print(configuration_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_sas_token(\n",
    "        block_blob_client, container_name, blob_name, permission, expiry=None,\n",
    "        timeout=None):\n",
    "    \"\"\"Create a blob sas token\n",
    "\n",
    "    :param block_blob_client: The storage block blob client to use.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the container to upload the blob to.\n",
    "    :param str blob_name: The name of the blob to upload the local file to.\n",
    "    :param expiry: The SAS expiry time.\n",
    "    :type expiry: `datetime.datetime`\n",
    "    :param int timeout: timeout in minutes from now for expiry,\n",
    "        will only be used if expiry is not specified\n",
    "    :return: A SAS token\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if expiry is None:\n",
    "        if timeout is None:\n",
    "            timeout = 60\n",
    "        expiry = datetime.datetime.utcnow() + datetime.timedelta(\n",
    "            minutes=timeout)\n",
    "    return block_blob_client.generate_blob_shared_access_signature(\n",
    "        container_name, blob_name, permission=permission, expiry=expiry)\n",
    "\n",
    "\n",
    "def upload_blob_and_create_sas(\n",
    "        block_blob_client, container_name, blob_name, file_name, expiry,\n",
    "        timeout=None):\n",
    "    \"\"\"Uploads a file from local disk to Azure Storage and creates\n",
    "    a SAS for it.\n",
    "\n",
    "    :param block_blob_client: The storage block blob client to use.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the container to upload the blob to.\n",
    "    :param str blob_name: The name of the blob to upload the local file to.\n",
    "    :param str file_name: The name of the local file to upload.\n",
    "    :param expiry: The SAS expiry time.\n",
    "    :type expiry: `datetime.datetime`\n",
    "    :param int timeout: timeout in minutes from now for expiry,\n",
    "        will only be used if expiry is not specified\n",
    "    :return: A SAS URL to the blob with the specified expiry time.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    block_blob_client.create_container(\n",
    "        container_name,\n",
    "        fail_on_exist=False)\n",
    "\n",
    "    block_blob_client.create_blob_from_path(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        file_name)\n",
    "\n",
    "    sas_token = create_sas_token(\n",
    "        block_blob_client,\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        permission=azureblob.BlobPermissions.READ,\n",
    "        expiry=expiry,\n",
    "        timeout=timeout)\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        sas_token=sas_token)\n",
    "\n",
    "    return sas_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pool(batch_client, block_blob_client, pool_id, vm_size, vm_count):\n",
    "    \"\"\"Creates an Azure Batch pool with the specified id.\n",
    "\n",
    "    :param batch_client: The batch client to use.\n",
    "    :type batch_client: `batchserviceclient.BatchServiceClient`\n",
    "    :param block_blob_client: The storage block blob client to use.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str pool_id: The id of the pool to create.\n",
    "    :param str vm_size: vm size (sku)\n",
    "    :param int vm_count: number of vms to allocate\n",
    "    \"\"\"\n",
    "    # pick the latest supported 16.04 sku for UbuntuServer\n",
    "    sku_to_use, image_ref_to_use = \\\n",
    "        common.helpers.select_latest_verified_vm_image_with_node_agent_sku(\n",
    "            batch_client, 'Canonical', 'UbuntuServer', '16.04')\n",
    "\n",
    "    block_blob_client.create_container(\n",
    "        _CONTAINER_NAME,\n",
    "        fail_on_exist=False)\n",
    "\n",
    "    sas_url = common.helpers.upload_blob_and_create_sas(\n",
    "        block_blob_client,\n",
    "        _CONTAINER_NAME,\n",
    "        _SIMPLE_TASK_NAME,\n",
    "        _SIMPLE_TASK_PATH,\n",
    "        datetime.datetime.utcnow() + datetime.timedelta(hours=1))\n",
    "\n",
    "    pool = batchmodels.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=image_ref_to_use,\n",
    "            node_agent_sku_id=sku_to_use),\n",
    "        vm_size=vm_size,\n",
    "        target_dedicated_nodes=vm_count,\n",
    "        start_task=batchmodels.StartTask(\n",
    "            command_line=\"python \" + _SIMPLE_TASK_NAME,\n",
    "            resource_files=[batchmodels.ResourceFile(\n",
    "                            file_path=_SIMPLE_TASK_NAME,\n",
    "                            blob_source=sas_url)]))\n",
    "\n",
    "    common.helpers.create_pool_if_not_exist(batch_client, pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\tgerrish\\\\Documents\\\\GitHub\\\\SAMAzure\\\\TestFiles\\\\case\\\\sky_mtx.json',\n",
       " 'C:\\\\Users\\\\tgerrish\\\\Documents\\\\GitHub\\\\SAMAzure\\\\TestFiles\\\\case\\\\surfaces.json',\n",
       " ['C:\\\\Users\\\\tgerrish\\\\Documents\\\\GitHub\\\\SAMAzure\\\\TestFiles\\\\case\\\\AnalysisGrids\\\\zone1.json',\n",
       "  'C:\\\\Users\\\\tgerrish\\\\Documents\\\\GitHub\\\\SAMAzure\\\\TestFiles\\\\case\\\\AnalysisGrids\\\\zone2.json'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "get_case_files(_CASE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.models as batchmodels\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory, extension):\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(extension):\n",
    "            files.append(os.path.abspath(os.path.join(directory, file)))\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def get_container_sas_token(block_blob_client, container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container, setting the expiry time and permissions. In this case, no start time is specified, so the shared access signature becomes valid immediately. Expiration is in 2 hours.\n",
    "    container_sas_token = block_blob_client.generate_container_shared_access_signature(container_name, permission=blob_permissions, expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=2))\n",
    "\n",
    "    return container_sas_token\n",
    "\n",
    "\n",
    "def get_container_sas_url(block_blob_client, container_name):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature URL that provides access to the ouput container to which the tasks will upload their output.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :rtype: str\n",
    "    :return: A SAS URL granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container.\n",
    "    sas_token = get_container_sas_token(block_blob_client, container_name, azureblob.BlobPermissions(read=True, write=True))\n",
    "\n",
    "    # Construct SAS URL for the container\n",
    "    container_sas_url = \"https://{}.blob.core.windows.net/{}?{}\".format(_STORAGE_ACCOUNT_NAME, container_name, sas_token)\n",
    "\n",
    "    return container_sas_url\n",
    "\n",
    "\n",
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "    \"\"\"\n",
    "    Uploads a local file to an Azure Blob storage container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param str file_path: The local path to the file.\n",
    "    :rtype: `azure.batch.models.ResourceFile`\n",
    "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch\n",
    "    tasks.\n",
    "    \"\"\"\n",
    "    blob_name = os.path.basename(file_path)\n",
    "\n",
    "    print('[{1:}] < {0:}'.format(os.path.relpath(file_path), container_name))\n",
    "\n",
    "    block_blob_client.create_blob_from_path(container_name, blob_name, file_path)\n",
    "\n",
    "    # Obtain the SAS token for the container.\n",
    "    sas_token = get_container_sas_token(block_blob_client, container_name, azureblob.BlobPermissions.READ)\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(container_name, blob_name, sas_token=sas_token)\n",
    "\n",
    "    return batchmodels.ResourceFile(file_path=blob_name, blob_source=sas_url)\n",
    "\n",
    "\n",
    "def wrap_commands_in_shell(ostype, commands):\n",
    "    \"\"\"\n",
    "    Wrap commands in a shell\n",
    "\n",
    "    :param list commands: list of commands to wrap\n",
    "    :param str ostype: OS type, linux or windows\n",
    "    :rtype: str\n",
    "    :return: a shell wrapping commands\n",
    "    \"\"\"\n",
    "    if ostype.lower() == \"linux\":\n",
    "        return \"/bin/bash -c \\\"set -e; set -o pipefail; {0:}; wait\\\"\".format(\";\".join(commands))\n",
    "    elif ostype.lower() == \"windows\":\n",
    "        return \"cmd.exe /c {0:}\".format(\"&\".join(commands))\n",
    "    else:\n",
    "        raise ValueError(\"unknown ostype: {}\".format(ostype))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def print_batch_exception(batch_exception):\n",
    "    \"\"\"\n",
    "    Prints the contents of the specified Batch exception.\n",
    "    :param batch_exception:\n",
    "    \"\"\"\n",
    "    print('-------------------------------------------')\n",
    "    print('Exception encountered:')\n",
    "    if (batch_exception.error and batch_exception.error.message and\n",
    "            batch_exception.error.message.value):\n",
    "        print(batch_exception.error.message.value)\n",
    "        if batch_exception.error.values:\n",
    "            print()\n",
    "            for mesg in batch_exception.error.values:\n",
    "                print('{}:\\t{}'.format(mesg.key, mesg.value))\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global keys\n",
    "\n",
    "_BATCH_ACCOUNT_NAME = \"climatebasedbatch\"\n",
    "_BATCH_ACCOUNT_KEY = \"W94ukoxG2neFkk6teOVZ3IQ8IQjmPJqPcFq48I9lLzCrPEQSRFS/+euaUEkkSyPoulUgnx5IEZxztA9574Hluw==\"\n",
    "_BATCH_ACCOUNT_URL = \"https://climatebasedbatch.westeurope.batch.azure.com\"\n",
    "\n",
    "_STORAGE_ACCOUNT_NAME = \"radfiles\"\n",
    "_STORAGE_ACCOUNT_KEY = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "\n",
    "_POOL_ID = \"batchpool\"\n",
    "_MIN_POOL_NODE = 1\n",
    "_MAX_POOL_NODE = 100\n",
    "\n",
    "_POOL_VM_SIZE = 'BASIC_A1'\n",
    "_NODE_OS_PUBLISHER = 'Canonical'\n",
    "_NODE_OS_OFFER = 'UbuntuServer'\n",
    "_NODE_OS_SKU = '16'\n",
    "\n",
    "_JOB_ID = \"0000000-testjob-3513\"\n",
    "_JOB_DIR = \"./case\"\n",
    "\n",
    "_LB_HB = \"./lb_hb.tar.gz\"\n",
    "_RADIANCE = \"./radiance-5.1.0-Linux.tar.gz\"\n",
    "_COPYBLOB = \"./copy_to_blob.py\"\n",
    "_SCRIPT = \"./RunHoneybeeRadiance.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000000-testjob-3513] blob container created.\n",
      "[0000000-common] blob container created.\n"
     ]
    }
   ],
   "source": [
    "# Create the blob client, for use in obtaining references to blob storage containers and uploading files to containers\n",
    "blob_client = azureblob.BlockBlobService(account_name=_STORAGE_ACCOUNT_NAME, account_key=_STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "# Create a job container\n",
    "blob_client.create_container(_JOB_ID, fail_on_exist=False)\n",
    "print('[{0:}] blob container created.'.format(_JOB_ID))\n",
    "\n",
    "# Create a common processing files container\n",
    "blob_client.create_container(\"0000000-common\", fail_on_exist=False)\n",
    "print('[{0:}] blob container created.'.format(\"0000000-common\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000000-testjob-3513] < case\\surfaces.json\n",
      "[0000000-testjob-3513] < case\\sky_mtx.json\n",
      "[0000000-testjob-3513] < case\\AnalysisGrids\\zone1.json\n",
      "[0000000-testjob-3513] < case\\AnalysisGrids\\zone2.json\n"
     ]
    }
   ],
   "source": [
    "# Upload case files to the blob\n",
    "\n",
    "_SURFACES_FILEPATH = os.path.abspath(os.path.join(_JOB_DIR, \"surfaces.json\"))\n",
    "surfaces_file = upload_file_to_container(blob_client, _JOB_ID, _SURFACES_FILEPATH)\n",
    "\n",
    "_SKY_MTX_FILEPATH = os.path.abspath(os.path.join(_JOB_DIR, \"sky_mtx.json\"))\n",
    "sky_mtx_file = upload_file_to_container(blob_client, _JOB_ID, _SKY_MTX_FILEPATH)\n",
    "\n",
    "_ANALYSIS_GRIDS_FILEPATHS = find_files(os.path.join(_JOB_DIR, \"AnalysisGrids\"), \"json\")\n",
    "analysis_grid_files = [upload_file_to_container(blob_client, _JOB_ID, file_path) for file_path in _ANALYSIS_GRIDS_FILEPATHS]\n",
    "\n",
    "# Get a number of files to be processed\n",
    "_POOL_NODE_COUNT = len(analysis_grid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000000-common] < radiance-5.1.0-Linux.tar.gz\n",
      "[0000000-common] < lb_hb.tar.gz\n",
      "[0000000-common] < RunHoneybeeRadiance.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output container SAS url created:\n",
      "https://radfiles.blob.core.windows.net/0000000-testjob-3513?se=2018-10-26T10%3A06%3A27Z&sp=rw&sv=2017-04-17&sr=c&sig=I8qXG08dhIt1632RtR0KGtXJGREQRBDG3Q6tc0sslY4%3D\n"
     ]
    }
   ],
   "source": [
    "# Get blob read/write credentials\n",
    "output_container_sas_url = get_container_sas_url(blob_client, _JOB_ID)\n",
    "print(\"Output container SAS url created:\\n{0:}\".format(output_container_sas_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batchpool] pool created\n"
     ]
    }
   ],
   "source": [
    "# Create a Batch service client. We'll now be interacting with the Batch service in addition to Storage\n",
    "credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME, _BATCH_ACCOUNT_KEY)\n",
    "batch_client = batch.BatchServiceClient(credentials, base_url=_BATCH_ACCOUNT_URL)\n",
    "\n",
    "# Create a pool ready to spin up some nodes\n",
    "start_commands = [\n",
    "    # Create a node with radiance, honeybee, the processing script anfd the copy to blob script available\n",
    "    \"touch ./do_i_exist.txt\",\n",
    "    \"cp -p do_i_exist.txt $AZ_BATCH_NODE_SHARED_DIR\",\n",
    "#     \"sudo apt-get update\",\n",
    "#     \"apt-get install rsync\"\n",
    "#     \"cp -p radiance-5.1.0-Linux.tar.gz\",\n",
    "#     \"tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "#     \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "#     \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "#     \"cp -p lb_hb.tar.gz $AZ_BATCH_NODE_SHARED_DIR\"\n",
    "#     \"tar xzf lb_hb.tar.gz\",\n",
    "]\n",
    "\n",
    "resources = [\n",
    "    radiance_file,\n",
    "    lb_hb_file,\n",
    "    run_process_file,\n",
    "    surfaces_file,\n",
    "    sky_mtx_file,\n",
    "]\n",
    "\n",
    "# Get the node agent SKU and image reference for the virtual machine configuration.\n",
    "sku_to_use, image_ref_to_use = select_latest_verified_vm_image_with_node_agent_sku(batch_client, _NODE_OS_PUBLISHER, _NODE_OS_OFFER, _NODE_OS_SKU)\n",
    "\n",
    "# Specify the user permissions and level\n",
    "user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "\n",
    "# Define the start task for the pool\n",
    "start_task = batch.models.StartTask(\n",
    "    command_line=wrap_commands_in_shell(\"linux\", start_commands),\n",
    "    user_identity=batchmodels.UserIdentity(auto_user=user),\n",
    "    wait_for_success=True,\n",
    "    resource_files=resources\n",
    ")\n",
    "\n",
    "# Define the pool\n",
    "new_pool = batch.models.PoolAddParameter(\n",
    "    id=_POOL_ID,\n",
    "    virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "        image_reference=image_ref_to_use,\n",
    "        node_agent_sku_id=sku_to_use\n",
    "    ),\n",
    "    vm_size=_POOL_VM_SIZE,\n",
    "    enable_auto_scale=True,\n",
    "    auto_scale_formula='pendingTaskSamplePercent =$PendingTasks.GetSamplePercent(180 * TimeInterval_Second);pendingTaskSamples = pendingTaskSamplePercent < 70 ? 1 : avg($PendingTasks.GetSample(180 * TimeInterval_Second)); $TargetDedicatedNodes = min(100, pendingTaskSamples);', \n",
    "    auto_scale_evaluation_interval=datetime.timedelta(minutes=5),\n",
    "    start_task=start_task,\n",
    ")\n",
    "\n",
    "# Try to create the pool, and tell us why not\n",
    "try:\n",
    "    batch_client.pool.add(new_pool)\n",
    "except batchmodels.batch_error.BatchErrorException as err:\n",
    "    print_batch_exception(err)\n",
    "    raise\n",
    "    \n",
    "print('[{0:}] pool created'.format(_POOL_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the job to which tasks will be assigned\n",
    "\n",
    "batch_client.job.add(batch.models.JobAddParameter(_JOB_ID, batch.models.PoolInformation(pool_id=_POOL_ID)))\n",
    "print('[{}] job created...'.format(_JOB_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tasks to the job\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tasks = []\n",
    "for idx, analysis_grid_file in enumerate(analysis_grid_files):\n",
    "    grid_file_path = analysis_grid_file.file_path\n",
    "    sky_mtx_file_path = sky_mtx_file.file_path\n",
    "    surfaces_file_path = surfaces_file.file_path\n",
    "    results_file_path = grid_file_path.replace(\".json\", \"_result.json\")\n",
    "\n",
    "    commands = [\n",
    "        \"apt-get update\", \"apt-get install wget rsync\"\n",
    "        \"wget https://github.com/NREL/Radiance/releases/download/5.1.0/radiance-5.1.0-Linux.tar.gz\",\n",
    "        \"tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "        \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "        \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "        \"tar xzf lb_hb.tar.gz\",\n",
    "        \"python3 RunHoneybeeRadiance.py -sm {0:} -s {1:} -p {2:}\".format(sky_mtx_file_path, surfaces_file_path, grid_file_path),\n",
    "    ]\n",
    "\n",
    "    command = wrap_commands_in_shell(\"linux\", commands)\n",
    "    \n",
    "    task_id = '{0:}_simulation'.format(re.sub(\"[^0-9a-zA-Z]\", \"\", grid_file_path.replace(\".json\", \"\")))\n",
    "\n",
    "    tasks.append(\n",
    "        batch.models.TaskAddParameter(\n",
    "            id=task_id,\n",
    "            command_line=command,\n",
    "            resource_files=[analysis_grid_file, sky_mtx_file, surfaces_file, lb_hb_file, run_process_file],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    results_file_path,\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            output_container_sas_url\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=batchmodels.OutputFileUploadOptions(\n",
    "                        batchmodels.OutputFileUploadCondition.task_success\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"Task [{0:}] created\".format(task_id))\n",
    "\n",
    "batch_tasks = batch_client.task.add_collection(_JOB_ID, tasks)\n",
    "\n",
    "print(\"Tasks added to job [{0:}]\".format(_JOB_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [\n",
    "    \"apt-get update\", \"apt-get install wget rsync\"\n",
    "    \"wget https://github.com/NREL/Radiance/releases/download/5.1.0/radiance-5.1.0-Linux.tar.gz\",\n",
    "    \"tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "    \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "    \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "    \"tar xzf lb_hb.tar.gz\",\n",
    "]\n",
    "\n",
    "command = wrap_commands_in_shell(\"linux\", commands)\n",
    "\n",
    "command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up a pool of nodes capable of running the case\n",
    "\n",
    "\n",
    "\n",
    "def create_pool(batch_service_client, pool_id, start_commands, resource_files, publisher, offer, sku, node_count):\n",
    "    \"\"\"\n",
    "    Creates a pool of compute nodes with the specified OS settings.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str pool_id: An ID for the new pool.\n",
    "    :param list resource_files: A collection of resource files for the pool's start task.\n",
    "    :param str publisher: Marketplace image publisher\n",
    "    :param str offer: Marketplace image offer\n",
    "    :param str sku: Marketplace image sku\n",
    "    \"\"\"\n",
    "    \n",
    "    start_task = batch.models.StartTask(command_line=wrap_commands_in_shell(\n",
    "        \"linux\",\n",
    "        start_commands),\n",
    "            user_identity=batchmodels.UserIdentity(auto_user=user),\n",
    "            wait_for_success=True,\n",
    "            resource_files=resource_files)\n",
    "    user = batchmodels.AutoUserSpecification(\n",
    "        scope=batchmodels.AutoUserScope.pool, \n",
    "        elevation_level=batchmodels.ElevationLevel.admin\n",
    "    )\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=image_ref_to_use,\n",
    "            node_agent_sku_id=sku_to_use),\n",
    "        vm_size=_POOL_VM_SIZE,\n",
    "        enable_auto_scale=True,\n",
    "        auto_scale_formula='pendingTaskSamplePercent =$PendingTasks.GetSamplePercent(180 * TimeInterval_Second);pendingTaskSamples = pendingTaskSamplePercent < 70 ? 1 : avg($PendingTasks.GetSample(180 * TimeInterval_Second)); $TargetDedicatedNodes = min(100, pendingTaskSamples);', \n",
    "        auto_scale_evaluation_interval=datetime.timedelta(minutes=5),\n",
    "        start_task=start_task,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        batch_service_client.pool.add(new_pool)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "    \n",
    "    print('[{0:}] pool created...'.format(pool_id))\n",
    "\n",
    "_POOL_NODE_COUNT = len(analysis_grid_files)\n",
    "\n",
    "pool = create_pool(batch_client, _POOL_ID, [], _NODE_OS_PUBLISHER, _NODE_OS_OFFER, _NODE_OS_SKU, _POOL_NODE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pool status\n",
    "\n",
    "if batch_client.pool.exists(_POOL_ID):\n",
    "    my_pool = batch_client.pool.get(_POOL_ID)\n",
    "    print(\"Current state: {}\".format(my_pool.allocation_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tasks to the job\n",
    "\n",
    "print('Adding {} tasks to job [{}]...'.format(len(analysis_grid_files), _JOB_ID))\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for idx, analysis_grid_file in enumerate(analysis_grid_files):\n",
    "    grid_file_path = analysis_grid_file.file_path\n",
    "    sky_mtx_file_path = sky_mtx_file.file_path\n",
    "    surfaces_file_path = surfaces_file.file_path\n",
    "    results_file_path = grid_file_path.replace(\".json\", \"_result.json\")\n",
    "\n",
    "    # Commands to be issued in each job\n",
    "    commands = [\n",
    "        \"apt-get update\",\n",
    "        \"apt-get install wget\",\n",
    "        \"apt-get install rsync\",\n",
    "        \"wget https://github.com/NREL/Radiance/releases/download/5.1.0/radiance-5.1.0-Linux.tar.gz\",\n",
    "        \"tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "        \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "        \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "        \"tar xzf lb_hb.tar.gz\",\n",
    "        \"python3 RunHoneybeeRadiance.py -sm {0:} -s {1:} -p {2:}\".format(sky_mtx_file_path, surfaces_file_path, grid_file_path),\n",
    "    ]\n",
    "\n",
    "    command = wrap_commands_in_shell(\"linux\", commands)\n",
    "\n",
    "    # print(command)\n",
    "\n",
    "    print()\n",
    "\n",
    "    tasks.append(\n",
    "        batch.models.TaskAddParameter(\n",
    "            id='task_{0:}'.format(re.sub(\"[^0-9a-zA-Z]\", \"\", grid_file_path.replace(\".json\", \"\"))),\n",
    "            command_line=command,\n",
    "            resource_files=[\n",
    "                analysis_grid_file,\n",
    "                sky_mtx_file,\n",
    "                surfaces_file,\n",
    "                lb_hb_file,\n",
    "                run_process_file,\n",
    "            ],\n",
    "            output_files=[\n",
    "                batchmodels.OutputFile(\n",
    "                    results_file_path,\n",
    "                    destination=batchmodels.OutputFileDestination(\n",
    "                        container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                            output_container_sas_url\n",
    "                        )\n",
    "                    ),\n",
    "                    upload_options=batchmodels.OutputFileUploadOptions(\n",
    "                        batchmodels.OutputFileUploadCondition.task_success\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "batch_tasks = batch_client.task.add_collection(_JOB_ID, tasks)\n",
    "\n",
    "print(batch_tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
