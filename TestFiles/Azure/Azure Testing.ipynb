{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -  Add a check to see if the container exists before uploading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "try:\n",
    "    input = raw_input\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import azure.batch.models as batchmodels\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "# import common.helpers  # noqa\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Batch and Storage account credential strings below with the values\n",
    "# unique to your accounts. These are used when constructing connection strings\n",
    "# for the Batch and Storage client objects.\n",
    "_BATCH_ACCOUNT_NAME = \"climatebasedbatch\"\n",
    "_BATCH_ACCOUNT_KEY = \"W94ukoxG2neFkk6teOVZ3IQ8IQjmPJqPcFq48I9lLzCrPEQSRFS/+euaUEkkSyPoulUgnx5IEZxztA9574Hluw==\"\n",
    "_BATCH_ACCOUNT_URL = \"https://climatebasedbatch.westeurope.batch.azure.com\"\n",
    "\n",
    "_STORAGE_ACCOUNT_NAME = \"radfiles\"\n",
    "_STORAGE_ACCOUNT_KEY = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "\n",
    "# Input the name for your job as well as the name of the pool you want to create.\n",
    "# Remember that for now it seems impossible to send more than 100 tasks per job\n",
    "# so I have set up some autoscaling capability lower down in the script. If you\n",
    "# more than 100 sims to run, the script will just create a new job with the same\n",
    "# name + i.\n",
    "# _JOB_ID =\n",
    "\n",
    "_POOL_ID = \"1st_deployment\"\n",
    "_MIN_POOL_NODE = 1\n",
    "_MAX_POOL_NODE = 100\n",
    "\n",
    "# Don't touch this stuff unless you know what you're doing! The startup commands\n",
    "# and task commands are written for bash in linux. You can maybe change the vm_size\n",
    "# but to be honest I find a normal room runs fine on the Basic A1 sized VM and it\n",
    "# costs Â£0.014/hour.\n",
    "_POOL_VM_SIZE = 'BASIC_A1'\n",
    "_NODE_OS_PUBLISHER = 'Canonical'\n",
    "_NODE_OS_OFFER = 'UbuntuServer'\n",
    "_NODE_OS_SKU = '16'\n",
    "\n",
    "# Scripts to be run to run different recipe types\n",
    "_RADIATION = \"honeybee_scripts/run_radiation.py\"\n",
    "_DAYLIGHT_FACTOR = \"honeybee_scripts/run_daylight_factor.py\"\n",
    "_ANNUAL = \"honeybee_scripts/run_annual.py\"\n",
    "_DAYLIGHT_COEFF = \"honeybee_scripts/run_daylight_coeff.py\"\n",
    "\n",
    "# Script to send outputs back to blob storage\n",
    "_COPY_TO_OUTPUT = 'copyToBlob.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the blob client\n",
    "For use in obtaining references to blob storage containers and uploading files to containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_client = azureblob.BlockBlobService(\n",
    "    account_name=_STORAGE_ACCOUNT_NAME,\n",
    "    account_key=_STORAGE_ACCOUNT_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload resource files into the blob\n",
    "<font size=\"5\" style=\"color:red\">This only really needs to be run each time the resource files change.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the file back to the blob\n",
    "import azure.storage.blob as azureblob\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Write a file containing something awesome!\n",
    "with open(\"test_output.txt\", \"w\") as f:\n",
    "    f.write(\"If you're reading this ... it worked!!! Also, Hello World!\")\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--filepath',\n",
    "    required=True,\n",
    "    help='The path to the text file to process. The path may include a compute node\\'s environment variables, such as $AZ_BATCH_NODE_SHARED_DIR/filename.txt'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--blobname',\n",
    "    required=True,\n",
    "    help='The full path the file should be saved to on theblob storage'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--storageaccount',\n",
    "    required=True,\n",
    "    help='The name the Azure Storage account that owns the blob storage container to which to upload results.'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--storagecontainer',\n",
    "    required=True,\n",
    "    help='The Azure Blob storage container to which to upload results.'\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--sastoken',\n",
    "    required=True,\n",
    "    help='The SAS token providing write access to the Storage container.'\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Create the blob client using the container's SAS token. This allows us to create a client that provides write access only to the container.\n",
    "blob_client = azureblob.BlockBlobService(\n",
    "    account_name=args.storageaccount,\n",
    "    sas_token=args.sastoken\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Uploading file {} to container [{}]...'.format(\n",
    "        args.blobname.rsplit(\"/\", 1)[1],\n",
    "        args.storagecontainer\n",
    "    )\n",
    ")\n",
    "\n",
    "blob_client.create_blob_from_path(\n",
    "    args.storagecontainer,\n",
    "    args.blobname,\n",
    "    args.filepath\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_BLOB_RESOURCE_CONTAINER = \"0000000-resources\"\n",
    "\n",
    "_RESOURCES = [\n",
    "    r\"C:\\Users\\tgerrish\\Desktop\\SAMAzure\\TestFiles\\FilePreparation\\honeybee\",\n",
    "    r\"C:\\Users\\tgerrish\\Desktop\\SAMAzure\\TestFiles\\FilePreparation\\ladybug\",\n",
    "    r\"C:\\Users\\tgerrish\\Desktop\\SAMAzure\\TestFiles\\FilePreparation\\02_run_radiance_case.py\",\n",
    "    r\"C:\\Users\\tgerrish\\Desktop\\SAMAzure\\TestFiles\\FilePreparation\\radiance-5.1.0-Linux.tar.gz\"\n",
    "]\n",
    "\n",
    "# Create resources blob location\n",
    "blob_client.create_container(_BLOB_RESOURCE_CONTAINER, fail_on_exist=False)\n",
    "\n",
    "time.sleep(10)  # Sleep for a bit to wait for Azure to create the container\n",
    "\n",
    "for obj in _RESOURCES:\n",
    "    # If the resource is a directory, zip and upload the zipped archive\n",
    "    if os.path.isdir(obj):\n",
    "        shutil.make_archive(\"{0:}\".format(obj), 'gztar', obj)\n",
    "        print(\"Uploading ../{0:}.tar.gz to container [{2:}/{1:}]\".format(os.path.basename(obj), os.path.basename(\"{0:}.tar.gz\".format(obj)), _BLOB_RESOURCE_CONTAINER))\n",
    "        blob_client.create_blob_from_path(_BLOB_RESOURCE_CONTAINER, os.path.basename(\"{0:}.tar.gz\".format(obj)), \"{0:}.tar.gz\".format(obj), timeout=5, max_connections=4)\n",
    "        os.remove(\"{0:}.tar.gz\".format(obj))\n",
    "    if os.path.isfile(obj):\n",
    "        print(\"Uploading ../{0:} to container [{1:}/{0:}]\".format(os.path.basename(obj), _BLOB_RESOURCE_CONTAINER))\n",
    "        blob_client.create_blob_from_path(_BLOB_RESOURCE_CONTAINER, os.path.basename(obj), obj, timeout=5, max_connections=4)\n",
    "\n",
    "print(\"\\nResource files uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data files\n",
    "This creates a container in blob storage in which the files to be processed by each of the tasks executed on the compute nodes in the pool.\n",
    "\n",
    "https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blockblobservice.blockblobservice?view=azure-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SOURCE_DIRECTORY = r\"C:\\Users\\tgerrish\\Desktop\\SAMAzure\\TestFiles\\FilePreparation\\0000000\"\n",
    "_BLOB_TARGET_CONTAINER = \"0000000-testproject-3513\"\n",
    "wait_time = 10\n",
    "\n",
    "# Create the top level container with the same name as the project\n",
    "blob_client.create_container(_BLOB_TARGET_CONTAINER, fail_on_exist=False)\n",
    "\n",
    "# Get the list of files within the source directory to upload\n",
    "source_paths = [item for sublist in [[os.path.join(path, name) for name in files] for path, subdirs, files in os.walk(_SOURCE_DIRECTORY)] for item in sublist]\n",
    "\n",
    "try:\n",
    "    # Upload the files\n",
    "    file_dict = {}\n",
    "    grid_dict = {}\n",
    "    for obj in source_paths:\n",
    "        if \"sky_mtx.json\" in obj:\n",
    "            a = blob_client.create_blob_from_path(_BLOB_TARGET_CONTAINER, obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), obj, timeout=5)\n",
    "            file_dict[\"sky_matrix\"] = a\n",
    "        if \"surfaces.json\" in obj:\n",
    "            a = blob_client.create_blob_from_path(_BLOB_TARGET_CONTAINER, obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), obj, timeout=5)\n",
    "            file_dict[\"surfaces\"] = a\n",
    "        if \"AnalysisGrid\" in obj:\n",
    "            a = blob_client.create_blob_from_path(_BLOB_TARGET_CONTAINER, obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), obj, timeout=5)\n",
    "            grid_dict[os.path.basename(obj).replace(\".json\", \"\")] = a\n",
    "        print(\"Uploading ../{0:} to container [{1:}/{0:}]\".format(obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), _BLOB_TARGET_CONTAINER))\n",
    "except:\n",
    "    print(\"Looks like the container hasn't been created just yet ... I'll wait for {0:} seconds ...\".format(wait_time))\n",
    "    \n",
    "    # Sleep for a bit to wait for Azure to create the container - If you get the error \"AzureMissingResourceHttpError: The specified container does not exist.\", try increasing this value\n",
    "    time.sleep(wait_time)\n",
    "    \n",
    "    # Upload the files\n",
    "    file_dict = {}\n",
    "    grid_dict = {}\n",
    "    for obj in source_paths:\n",
    "        if \"sky_mtx.json\" in obj:\n",
    "            a = blob_client.create_blob_from_path(_BLOB_TARGET_CONTAINER, obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), obj, timeout=5)\n",
    "            file_dict[\"sky_matrix\"] = a\n",
    "        if \"surfaces.json\" in obj:\n",
    "            a = blob_client.create_blob_from_path(_BLOB_TARGET_CONTAINER, obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), obj, timeout=5)\n",
    "            file_dict[\"surfaces\"] = a\n",
    "        if \"AnalysisGrid\" in obj:\n",
    "            a = blob_client.create_blob_from_path(_BLOB_TARGET_CONTAINER, obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), obj, timeout=5)\n",
    "            grid_dict[os.path.basename(obj).replace(\".json\", \"\")] = a\n",
    "        print(\"Uploading ../{0:} to container [{1:}/{0:}]\".format(obj.replace(\"{0:}\\\\\".format(_SOURCE_DIRECTORY), \"\").replace(\"\\\\\", \"/\"), _BLOB_TARGET_CONTAINER))\n",
    "\n",
    "print(\"\\nResource files uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get credentials for writing data to the project blob container\n",
    "Obtains a shared access signature that provides write access to the output container to which the tasks will upload their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'se=2018-08-07T07%3A36%3A12Z&sp=w&sv=2017-04-17&sr=c&sig=eaaX/v3HGfnmS2SQQH7OjqQCF1b5w0ZY12DhxD%2BzBe8%3D'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_container_sas_token = blob_client.generate_container_shared_access_signature(\n",
    "            _BLOB_TARGET_CONTAINER,\n",
    "            permission=azureblob.BlobPermissions.WRITE,\n",
    "            expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24)\n",
    ")\n",
    "\n",
    "output_container_sas_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Batch service client\n",
    "We'll now be interacting with the Batch service in addition to Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME, _BATCH_ACCOUNT_KEY)\n",
    "                                             \n",
    "batch_client = batch.BatchServiceClient(credentials, base_url=_BATCH_ACCOUNT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'application_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-48831090c9f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m create_pool(batch_client,\n\u001b[0;32m     93\u001b[0m                 \u001b[0m_POOL_ID\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mapplication_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0m_NODE_OS_PUBLISHER\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0m_NODE_OS_OFFER\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'application_files' is not defined"
     ]
    }
   ],
   "source": [
    "def create_pool(batch_service_client, pool_id,\n",
    "                resource_files, publisher, offer, sku):\n",
    "    \"\"\"\n",
    "    Creates a pool of compute nodes with the specified OS settings.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str pool_id: An ID for the new pool.\n",
    "    :param list resource_files: A collection of resource files for the pool's\n",
    "    start task.\n",
    "    :param str publisher: Marketplace image publisher\n",
    "    :param str offer: Marketplace image offer\n",
    "    :param str sku: Marketplace image sku\n",
    "    \"\"\"\n",
    "    print('Creating pool [{}]...'.format(pool_id))\n",
    "\n",
    "    # Create a new pool of Linux compute nodes using an Azure Virtual Machines\n",
    "    # Marketplace image. For more information about creating pools of Linux\n",
    "    # nodes, see:\n",
    "    # https://azure.microsoft.com/documentation/articles/batch-linux-nodes/\n",
    "\n",
    "    # Specify the commands for the pool's start task. The start task is run\n",
    "    # on each node as it joins the pool, and when it's rebooted or re-imaged.\n",
    "    # We use the start task to prep the node for running our task script.\n",
    "    task_commands = [\n",
    "        # Copy the python_tutorial_task.py script to the \"shared\" directory\n",
    "        # that all tasks that run on the node have access to. Note that\n",
    "        # we are using the -p flag with cp to preserve the file uid/gid,\n",
    "        # otherwise since this start task is run as an admin, it would not\n",
    "        # be accessible by tasks run as a non-admin user.\n",
    "        # 'cp -p {} $AZ_BATCH_NODE_SHARED_DIR'.format(_BAT_TO_BASH),\n",
    "        'cp -p {} $AZ_BATCH_NODE_SHARED_DIR'.format(_COPY_TO_OUTPUT),\n",
    "        ############################\n",
    "        # Install the relevant files\n",
    "        ############################\n",
    "        # Update Ubuntu\n",
    "        #\"apt-get update\",\n",
    "        # Install git\n",
    "        #\"apt-get install git-core -y\",\n",
    "        # Install rsync\n",
    "        #\"apt-get install rsync -y\"\n",
    "        # Install pip\n",
    "        \"apt-get install python-pip -y\"\n",
    "        # Install the azure-storage module so that the task script can access Azure Blob storage, pre-cryptography version\n",
    "        'pip install azure-storage==0.32.0',\n",
    "        # Install Radiance\n",
    "        #\"curl -fsSL https://github.com/NREL/Radiance/releases/download/5.1.0/radiance-5.1.0-Linux.tar.gz -o radiance-5.1.0-Linux.tar.gz\",\n",
    "        #\"tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "        #\"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "        #\"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/ --remove-source-files\"\n",
    "        # Copy honeybee ladybug zip from github\n",
    "        \n",
    "        # Install docker\n",
    "#         'sudo apt-get install docker -y && sudo apt-get install docker.io -y',\n",
    "        # Pull RadHoneyWhale from docker hub\n",
    "#         'sudo docker pull antoinedao/radhoneywhale',\n",
    "        # Install git\n",
    "        #\"sudo apt-get install git\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Get the node agent SKU and image reference for the virtual machine\n",
    "    # configuration.\n",
    "    # For more information about the virtual machine configuration, see:\n",
    "    # https://azure.microsoft.com/documentation/articles/batch-linux-nodes/\n",
    "    sku_to_use, image_ref_to_use = \\\n",
    "        common.helpers.select_latest_verified_vm_image_with_node_agent_sku(\n",
    "            batch_service_client, publisher, offer, sku)\n",
    "    user = batchmodels.AutoUserSpecification(\n",
    "        scope=batchmodels.AutoUserScope.pool,\n",
    "        elevation_level=batchmodels.ElevationLevel.admin)\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=image_ref_to_use,\n",
    "            node_agent_sku_id=sku_to_use),\n",
    "        vm_size=_POOL_VM_SIZE,\n",
    "        resize_timeout=datetime.timedelta(minutes=15),\n",
    "        target_dedicated_nodes=_POOL_NODE_COUNT,\n",
    "        start_task=batch.models.StartTask(\n",
    "            command_line=common.helpers.wrap_commands_in_shell('linux',\n",
    "                                                               task_commands),\n",
    "            user_identity=batchmodels.UserIdentity(auto_user=user),\n",
    "            wait_for_success=True,\n",
    "            resource_files=resource_files),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        batch_service_client.pool.add(new_pool)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "        \n",
    "create_pool(batch_client,\n",
    "                _POOL_ID,\n",
    "                application_files,\n",
    "                _NODE_OS_PUBLISHER,\n",
    "                _NODE_OS_OFFER,\n",
    "                _NODE_OS_SKU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se=2018-08-06T09%3A06%3A20Z&sp=r&sv=2017-04-17&sr=c&sig=ASzEKW2e0Jq0PHgpYwhyO8hmRys5PrlfJCA3gXPuiHQ%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.storage.blob.models.Blob at 0x1d8999e2b70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy file from the blob to a host machine\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import (\n",
    "    BlockBlobService,\n",
    "    ContainerPermissions,\n",
    ")\n",
    "\n",
    "accountName = \"radfiles\"\n",
    "accountKey = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "containerName = \"0000000-testproject-3513\"\n",
    "\n",
    "blobService = BlockBlobService(account_name=accountName, account_key=accountKey)\n",
    "sas_token = blobService.generate_container_shared_access_signature(containerName,ContainerPermissions.READ, datetime.utcnow() + timedelta(hours=1))\n",
    "print(sas_token)\n",
    "\n",
    "blobService = BlockBlobService(account_name=accountName, account_key=None, sas_token=sas_token)\n",
    "blobService.get_blob_to_path(containerName, \"test.py\", \"_00_testING.py\")\n",
    "          \n",
    "    \n",
    "# _BATCH_ACCOUNT_NAME = \"climatebasedbatch\"\n",
    "# _BATCH_ACCOUNT_KEY = \"W94ukoxG2neFkk6teOVZ3IQ8IQjmPJqPcFq48I9lLzCrPEQSRFS/+euaUEkkSyPoulUgnx5IEZxztA9574Hluw==\"\n",
    "# _BATCH_ACCOUNT_URL = \"https://climatebasedbatch.westeurope.batch.azure.com\"\n",
    "\n",
    "# _STORAGE_ACCOUNT_NAME = \"radfiles\"\n",
    "# _STORAGE_ACCOUNT_KEY = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from host machine to the Blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "    \"\"\"\n",
    "    Uploads a local file to an Azure Blob storage container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param str file_path: The local path to the file.\n",
    "    :rtype: `azure.batch.models.ResourceFile`\n",
    "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch\n",
    "    tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    import datetime\n",
    "    import azure.storage.blob as azureblob\n",
    "    import azure.batch.models as batchmodels\n",
    "\n",
    "    blob_name = os.path.basename(file_path)\n",
    "    #blob_name = \"/\".join(os.path.split(file_path))\n",
    "\n",
    "    print('Uploading file {} to container [{}]...'.format(blob_name,\n",
    "                                                          container_name))\n",
    "\n",
    "    block_blob_client.create_blob_from_path(container_name,\n",
    "                                            blob_name,\n",
    "                                            file_path)\n",
    "\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        permission=azureblob.BlobPermissions.READ,\n",
    "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24))\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(container_name,\n",
    "                                              blob_name,\n",
    "                                              sas_token=sas_token)\n",
    "\n",
    "    return batchmodels.ResourceFile(file_path=blob_name,\n",
    "                                    blob_source=sas_url)\n",
    "\n",
    "def get_container_sas_token(block_blob_client,\n",
    "                            container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the\n",
    "    container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container, setting the expiry time and\n",
    "    # permissions. In this case, no start time is specified, so the shared\n",
    "    # access signature becomes valid immediately.\n",
    "    container_sas_token = \\\n",
    "        block_blob_client.generate_container_shared_access_signature(\n",
    "            container_name,\n",
    "            permission=blob_permissions,\n",
    "            expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24))\n",
    "\n",
    "    return container_sas_token\n",
    "\n",
    "def create_pool(batch_service_client, pool_id,\n",
    "                resource_files, publisher, offer, sku):\n",
    "    \"\"\"\n",
    "    Creates a pool of compute nodes with the specified OS settings.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str pool_id: An ID for the new pool.\n",
    "    :param list resource_files: A collection of resource files for the pool's\n",
    "    start task.\n",
    "    :param str publisher: Marketplace image publisher\n",
    "    :param str offer: Marketplace image offer\n",
    "    :param str sku: Marketplace image sku\n",
    "    \"\"\"\n",
    "    print('Creating pool [{}]...'.format(pool_id))\n",
    "\n",
    "    # Create a new pool of Linux compute nodes using an Azure Virtual Machines\n",
    "    # Marketplace image. For more information about creating pools of Linux\n",
    "    # nodes, see:\n",
    "    # https://azure.microsoft.com/documentation/articles/batch-linux-nodes/\n",
    "\n",
    "    # Specify the commands for the pool's start task. The start task is run\n",
    "    # on each node as it joins the pool, and when it's rebooted or re-imaged.\n",
    "    # We use the start task to prep the node for running our task script.\n",
    "    task_commands = [\n",
    "        # Copy the python_tutorial_task.py script to the \"shared\" directory\n",
    "        # that all tasks that run on the node have access to. Note that\n",
    "        # we are using the -p flag with cp to preserve the file uid/gid,\n",
    "        # otherwise since this start task is run as an admin, it would not\n",
    "        # be accessible by tasks run as a non-admin user.\n",
    "        # 'cp -p {} $AZ_BATCH_NODE_SHARED_DIR'.format(_BAT_TO_BASH),\n",
    "        'cp -p {} $AZ_BATCH_NODE_SHARED_DIR'.format(_COPY_TO_OUTPUT),\n",
    "        # Install pip\n",
    "        'curl -fSsL https://bootstrap.pypa.io/get-pip.py | python',\n",
    "        # Install the azure-storage module so that the task script can access\n",
    "        # Azure Blob storage, pre-cryptography version\n",
    "        'pip install azure-storage==0.32.0',\n",
    "        # Install docker\n",
    "        'sudo apt-get install docker -y && sudo apt-get install docker.io -y',\n",
    "        # Pull RadHoneyWhale from docker hub\n",
    "        'sudo docker pull antoinedao/radhoneywhale']\n",
    "\n",
    "\n",
    "    # Get the node agent SKU and image reference for the virtual machine\n",
    "    # configuration.\n",
    "    # For more information about the virtual machine configuration, see:\n",
    "    # https://azure.microsoft.com/documentation/articles/batch-linux-nodes/\n",
    "    sku_to_use, image_ref_to_use = \\\n",
    "        common.helpers.select_latest_verified_vm_image_with_node_agent_sku(\n",
    "            batch_service_client, publisher, offer, sku)\n",
    "    user = batchmodels.AutoUserSpecification(\n",
    "        scope=batchmodels.AutoUserScope.pool,\n",
    "        elevation_level=batchmodels.ElevationLevel.admin)\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=image_ref_to_use,\n",
    "            node_agent_sku_id=sku_to_use),\n",
    "        vm_size=_POOL_VM_SIZE,\n",
    "        resize_timeout=datetime.timedelta(minutes=15),\n",
    "        target_dedicated_nodes=_POOL_NODE_COUNT,\n",
    "        start_task=batch.models.StartTask(\n",
    "            command_line=common.helpers.wrap_commands_in_shell('linux',\n",
    "                                                               task_commands),\n",
    "            user_identity=batchmodels.UserIdentity(auto_user=user),\n",
    "            wait_for_success=True,\n",
    "            resource_files=resource_files),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        batch_service_client.pool.add(new_pool)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "def create_job(batch_service_client, job_id, pool_id):\n",
    "    \"\"\"\n",
    "    Creates a job with the specified ID, associated with the specified pool.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID for the job.\n",
    "    :param str pool_id: The ID for the pool.\n",
    "    \"\"\"\n",
    "    print('Creating job [{}]...'.format(job_id))\n",
    "\n",
    "    job = batch.models.JobAddParameter(\n",
    "        job_id,\n",
    "        batch.models.PoolInformation(pool_id=pool_id))\n",
    "\n",
    "    try:\n",
    "        batch_service_client.job.add(job)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "def add_tasks(batch_service_client, job_id, input_files,\n",
    "              input_file_names, output_container_name, output_container_sas_token):\n",
    "    \"\"\"\n",
    "    Adds a task for each input file in the collection to the specified job.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID of the job to which to add the tasks.\n",
    "    :param list input_files: A collection of input files. One task will be\n",
    "     created for each input file.\n",
    "    :param output_container_name: The ID of an Azure Blob storage container to\n",
    "    which the tasks will upload their results.\n",
    "    :param output_container_sas_token: A SAS token granting write access to\n",
    "    the specified Azure Blob storage container.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n",
    "\n",
    "    user = batchmodels.UserIdentity(\n",
    "    auto_user=batchmodels.AutoUserSpecification(\n",
    "        elevation_level=batchmodels.ElevationLevel.admin,\n",
    "        scope=batchmodels.AutoUserScope.task))\n",
    "\n",
    "    tasks = list()\n",
    "\n",
    "    for idx, input_file in enumerate(input_files):\n",
    "        filename = input_file_names[idx].split(\".\")[0]\n",
    "\n",
    "        command = [ 'sudo bash',\n",
    "                    'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job:z antoinedao/radhoneywhale unzip /usr/job/{} -d /usr/job/'.format(input_file_names[idx]),\n",
    "                    'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale python3 /usr/convertToBash.py '\n",
    "                    '--filepath /usr/job',\n",
    "                    'sudo chmod +x $AZ_BATCH_TASK_WORKING_DIR/gridbased_annual/commands.sh',\n",
    "                    'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale /usr/job/gridbased_annual/commands.sh',\n",
    "                    'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale zip -r /usr/job/out_{} /usr/job/'.format(filename + \".zip\"),\n",
    "                    'python $AZ_BATCH_NODE_SHARED_DIR/copyToBlob.py '\n",
    "                    '--filepath $AZ_BATCH_TASK_WORKING_DIR/{} --filename {} --storageaccount {} '\n",
    "                    '--storagecontainer {} --sastoken \"{}\"'.format(\n",
    "                     'out_'+filename + \".zip\",\n",
    "                     'out_'+filename + \".zip\",\n",
    "                     _STORAGE_ACCOUNT_NAME,\n",
    "                     output_container_name,\n",
    "                     output_container_sas_token)]\n",
    "\n",
    "        tasks.append(batch.models.TaskAddParameter(\n",
    "                'Task_{}'.format(idx,filename),\n",
    "                common.helpers.wrap_commands_in_shell('linux', command),\n",
    "                resource_files=[input_file],\n",
    "                user_identity=user\n",
    "                )\n",
    "        )\n",
    "\n",
    "    batch_service_client.task.add_collection(job_id, tasks)\n",
    "\n",
    "def wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n",
    "    \"\"\"\n",
    "    Returns when all tasks in the specified job reach the Completed state.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The id of the job whose tasks should be to monitored.\n",
    "    :param timedelta timeout: The duration to wait for task completion. If all\n",
    "    tasks in the specified job do not reach Completed state within this time\n",
    "    period, an exception will be raised.\n",
    "    \"\"\"\n",
    "    timeout_expiration = datetime.datetime.now() + timeout\n",
    "\n",
    "    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\"\n",
    "          .format(timeout), end='')\n",
    "\n",
    "    while datetime.datetime.now() < timeout_expiration:\n",
    "        print('.', end='')\n",
    "        sys.stdout.flush()\n",
    "        tasks = [batch_service_client.task.list(job) for job in job_id]\n",
    "\n",
    "        tasks = [item for sublist in tasks for item in sublist]\n",
    "\n",
    "        incomplete_tasks = [task for task in tasks if\n",
    "                            task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    print()\n",
    "    raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within \"\n",
    "                       \"timeout period of \" + str(timeout))\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def download_blobs_from_container(block_blob_client,\n",
    "                                  container_name, directory_path):\n",
    "    \"\"\"\n",
    "    Downloads all blobs from the specified Azure Blob storage container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param container_name: The Azure Blob storage container from which to\n",
    "     download files.\n",
    "    :param directory_path: The local directory to which to download the files.\n",
    "    \"\"\"\n",
    "    print('Downloading all files from container [{}]...'.format(\n",
    "        container_name))\n",
    "\n",
    "    container_blobs = block_blob_client.list_blobs(container_name)\n",
    "\n",
    "    for blob in container_blobs.items:\n",
    "        destination_file_path = os.path.join(directory_path, blob.name)\n",
    "\n",
    "        block_blob_client.get_blob_to_path(container_name,\n",
    "                                           blob.name,\n",
    "                                           destination_file_path)\n",
    "\n",
    "        print('  Downloaded blob [{}] from container [{}] to {}'.format(\n",
    "            blob.name,\n",
    "            container_name,\n",
    "            destination_file_path))\n",
    "\n",
    "    print('  Download complete!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # First thing to do is Zip all of your files so they won't take too much space\n",
    "    # when uploading. It also makes it easier to manipulate upload behaviour\n",
    "    zipped_directory = _DIRECTORY + \"_zipped\"\n",
    "    print(\"Zipping {} folders into {}\".format(len(os.listdir(_DIRECTORY)),zipped_directory))\n",
    "    print(\"Go get yourself a cuppa, this can take some time...\")\n",
    "\n",
    "    for name in os.listdir(_DIRECTORY):\n",
    "       shutil.make_archive(os.path.join(zipped_directory,name),'zip', os.path.join(_DIRECTORY,name))\n",
    "\n",
    "    # Create the blob client, for use in obtaining references to\n",
    "    # blob storage containers and uploading files to containers.\n",
    "    blob_client = azureblob.BlockBlobService(\n",
    "        account_name=_STORAGE_ACCOUNT_NAME,\n",
    "        account_key=_STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "    # Use the blob client to create the containers in Azure Storage if they\n",
    "    # don't yet exist.\n",
    "    APP_CONTAINER_NAME = 'application'\n",
    "    INPUT_CONTAINER_NAME = 'input'\n",
    "    OUTPUT_CONTAINER_NAME = 'output'\n",
    "    blob_client.create_container(APP_CONTAINER_NAME, fail_on_exist=False)\n",
    "    blob_client.create_container(INPUT_CONTAINER_NAME, fail_on_exist=False)\n",
    "    blob_client.create_container(OUTPUT_CONTAINER_NAME, fail_on_exist=False)\n",
    "\n",
    "    # Paths to the task script. This script will be executed by the tasks that\n",
    "    # run on the compute nodes.\n",
    "    application_file_paths = [os.path.realpath('copyToBlob.py')]\n",
    "\n",
    "    # Upload the application script to Azure Storage. This is the script that\n",
    "    # will process the data files, and is executed by each of the tasks on the\n",
    "    # compute nodes.\n",
    "    application_files = [\n",
    "        upload_file_to_container(blob_client, APP_CONTAINER_NAME, file_path)\n",
    "        for file_path in application_file_paths]\n",
    "\n",
    "    # The collection of data files that are to be processed by the tasks.\n",
    "    input_file_paths = list()\n",
    "\n",
    "    for path in os.listdir(zipped_directory):\n",
    "        input_file_paths.append(os.path.join(zipped_directory,path))\n",
    "\n",
    "    input_file_name = [os.path.basename(input_file_path).split(\".\")[0] for input_file_path in\n",
    "                       input_file_paths]\n",
    "\n",
    "    print(input_file_name)\n",
    "\n",
    "\n",
    "    # Upload the data files. This is the data that will be processed by each of\n",
    "    # the tasks executed on the compute nodes in the pool.\n",
    "    input_files = [\n",
    "        upload_file_to_container(blob_client, INPUT_CONTAINER_NAME, file_path)\n",
    "        for file_path in input_file_paths]\n",
    "\n",
    "    # Obtain a shared access signature that provides write access to the output\n",
    "    # container to which the tasks will upload their output.\n",
    "    output_container_sas_token = get_container_sas_token(\n",
    "        blob_client,\n",
    "        OUTPUT_CONTAINER_NAME,\n",
    "        azureblob.BlobPermissions.WRITE)\n",
    "\n",
    "    # Create a Batch service client. We'll now be interacting with the Batch\n",
    "    # service in addition to Storage.\n",
    "    credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME,\n",
    "                                                 _BATCH_ACCOUNT_KEY)\n",
    "\n",
    "    batch_client = batch.BatchServiceClient(\n",
    "        credentials,\n",
    "        base_url=_BATCH_ACCOUNT_URL)\n",
    "\n",
    "    # Create the pool that will contain the compute nodes that will execute the\n",
    "    # tasks. The resource files we pass in are used for configuring the pool's\n",
    "    # start task, which is executed each time a node first joins the pool (or\n",
    "    # # is rebooted or re-imaged).\n",
    "    create_pool(batch_client,\n",
    "                _POOL_ID,\n",
    "                application_files,\n",
    "                _NODE_OS_PUBLISHER,\n",
    "                _NODE_OS_OFFER,\n",
    "                _NODE_OS_SKU)\n",
    "\n",
    "    jobs = list()\n",
    "\n",
    "    for i, (files,names) in enumerate(zip(list(chunks(input_files, 100)),list(chunks(input_file_name, 100)))):\n",
    "        job_id = _JOB_ID + \"_\" + str(i)\n",
    "        jobs.append(job_id)\n",
    "\n",
    "        # Create the job that will run the tasks.\n",
    "        create_job(batch_client, job_id, _POOL_ID)\n",
    "\n",
    "        # Add the tasks to the job. We need to supply a container shared access\n",
    "        # signature (SAS) token for the tasks so that they can upload their output\n",
    "        # to Azure Storage.\n",
    "\n",
    "        add_tasks(batch_client,\n",
    "                  job_id,\n",
    "                  files,\n",
    "                  names,\n",
    "                  OUTPUT_CONTAINER_NAME,\n",
    "                  output_container_sas_token)\n",
    "\n",
    "\n",
    "    # Pause execution until tasks reach Completed state.\n",
    "    wait_for_tasks_to_complete(batch_client,\n",
    "                               jobs,\n",
    "                               datetime.timedelta(hours=24))\n",
    "\n",
    "    print(\"  Success! All tasks reached the 'Completed' state within the \"\n",
    "          \"specified timeout period.\")\n",
    "\n",
    "    # Download the task output files from the output Storage container to a\n",
    "    # local directory. Note that we could have also downloaded the output\n",
    "    # files directly from the compute nodes themselves.\n",
    "    download_blobs_from_container(blob_client,\n",
    "                                  output_container_name,\n",
    "                                  os.path.expanduser('~'))\n",
    "\n",
    "    # Clean up storage resources\n",
    "    print('Deleting containers...')\n",
    "    blob_client.delete_container(app_container_name)\n",
    "    blob_client.delete_container(input_container_name)\n",
    "    blob_client.delete_container(output_container_name)\n",
    "\n",
    "    # Print out some timing info\n",
    "    end_time = datetime.datetime.now().replace(microsecond=0)\n",
    "    print()\n",
    "    print('Sample end: {}'.format(end_time))\n",
    "    print('Elapsed time: {}'.format(end_time - start_time))\n",
    "    print()\n",
    "\n",
    "\n",
    "    #if query_yes_no('Delete pool?') == 'yes':\n",
    "    batch_client.pool.delete(_POOL_ID)\n",
    "\n",
    "    # Clean up Batch resources (if the user so chooses).\n",
    "    if query_yes_no('Delete job?') == 'yes':\n",
    "        for job in jobs:\n",
    "            batch_client.job.delete(job)\n",
    "\n",
    "    print()\n",
    "    input('Press ENTER to exit...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
