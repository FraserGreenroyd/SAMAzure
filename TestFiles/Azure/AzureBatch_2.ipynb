{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob client generated:  <azure.storage.blob.blockblobservice.BlockBlobService object at 0x000002A3C40F89B0>\n",
      "\n",
      "Batch client generated: <azure.batch.batch_service_client.BatchServiceClient object at 0x000002A3C41364A8>\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'azure.storage.blob' has no attribute 'blob_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-1244a8bd11a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;31m# Generate a sas token and url capable of read/write\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m container_sas_token = azureblob.blob_client.generate_container_shared_access_signature(\n\u001b[0m\u001b[0;32m    149\u001b[0m     \u001b[0m_JOB_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     permission=azureblob.BlobPermissions(\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'azure.storage.blob' has no attribute 'blob_client'"
     ]
    }
   ],
   "source": [
    "#######################################################################################################\n",
    "# Imports\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.models as batchmodels\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#######################################################################################################\n",
    "# Global keys\n",
    "\n",
    "_BATCH_ACCOUNT_NAME = \"climatebasedbatch\"\n",
    "_BATCH_ACCOUNT_KEY = \"W94ukoxG2neFkk6teOVZ3IQ8IQjmPJqPcFq48I9lLzCrPEQSRFS/+euaUEkkSyPoulUgnx5IEZxztA9574Hluw==\"\n",
    "_BATCH_ACCOUNT_URL = \"https://climatebasedbatch.westeurope.batch.azure.com\"\n",
    "\n",
    "_STORAGE_ACCOUNT_NAME = \"radfiles\"\n",
    "_STORAGE_ACCOUNT_KEY = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "\n",
    "_POOL_ID = \"batchpool\"\n",
    "_MIN_POOL_NODE = 1\n",
    "_MAX_POOL_NODE = 100\n",
    "\n",
    "_POOL_VM_SIZE = 'BASIC_A1'\n",
    "_NODE_OS_PUBLISHER = 'Canonical'\n",
    "_NODE_OS_OFFER = 'UbuntuServer'\n",
    "_NODE_OS_SKU = '16'\n",
    "\n",
    "_JOB_NAME = \"0000000-testjob-3513\"\n",
    "_JOB_DIRECTORY = \"./radfiles\"\n",
    "_COPY_TO_BLOB_LOCAL = \"./docker/copy_to_blob.py\"\n",
    "\n",
    "#######################################################################################################\n",
    "# Create the blob client - for use in managing blob storage containers and upload/download transactions\n",
    "\n",
    "blob_client = azureblob.BlockBlobService(\n",
    "    account_name=_STORAGE_ACCOUNT_NAME, \n",
    "    account_key=_STORAGE_ACCOUNT_KEY\n",
    ")\n",
    "print(\"Blob client generated:  {0:}\\n\".format(blob_client))\n",
    "\n",
    "#######################################################################################################\n",
    "# Create the batch client - for use in transacting with batch pool and nodes\n",
    "\n",
    "batch_client = batch.BatchServiceClient(\n",
    "    batchauth.SharedKeyCredentials(\n",
    "        _BATCH_ACCOUNT_NAME, \n",
    "        _BATCH_ACCOUNT_KEY\n",
    "    ), \n",
    "    base_url=_BATCH_ACCOUNT_URL\n",
    ")\n",
    "print(\"Batch client generated: {0:}\\n\".format(batch_client))\n",
    "\n",
    "#######################################################################################################\n",
    "# Create container for job file storage\n",
    "\n",
    "# container = blob_client.create_container(\n",
    "#     _JOB_NAME, \n",
    "#     fail_on_exist=False\n",
    "# )\n",
    "# print(\"Container created: [{0:}/{1:}]\\n\".format(_STORAGE_ACCOUNT_NAME, _JOB_NAME))\n",
    "\n",
    "#######################################################################################################\n",
    "# Define the fucntions used to find files and uplaod them to the blob from the host\n",
    "\n",
    "def directory_files(directory):\n",
    "    \"\"\"\n",
    "    Generates a list of the files within a directory\n",
    "    :param str directory: A path to a directory.\n",
    "    :return list: List of files\n",
    "    \"\"\"\n",
    "    return [item for sublist in [[os.path.join(path, name) for name in files] for path, subdirs, files in os.walk(directory)] for item in sublist]\n",
    "\n",
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "    \"\"\"\n",
    "    Uploads a local file to an Azure Blob storage container.\n",
    "    :param block_blob_client: An Azure blockblobservice client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param str file_path: The local path to the file.\n",
    "    :rtype: `azure.batch.models.ResourceFile`\n",
    "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch tasks.\n",
    "    \"\"\"\n",
    "    blob_name = os.path.basename(file_path)\n",
    "    print('Uploading file {0:} to container [{1:}/{0:}]'.format(blob_name, container_name))\n",
    "    block_blob_client.create_blob_from_path(\n",
    "        container_name, \n",
    "        blob_name, \n",
    "        file_path\n",
    "    )\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(\n",
    "        container_name, \n",
    "        blob_name, \n",
    "#         permission=azureblob.BlobPermissions.READ, \n",
    "        permission=azureblob.BlobPermissions(\n",
    "            read=True, \n",
    "            add=False, \n",
    "            create=False, \n",
    "            write=False, \n",
    "            delete=False, \n",
    "            _str=None\n",
    "        ), \n",
    "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24)\n",
    "    )\n",
    "    sas_url = block_blob_client.make_blob_url(\n",
    "        container_name, \n",
    "        blob_name, \n",
    "        sas_token=sas_token\n",
    "    )\n",
    "    return batchmodels.ResourceFile(file_path=blob_name, blob_source=sas_url)\n",
    "\n",
    "#######################################################################################################\n",
    "# Upload the files common to all simulations (sky_mtx.json, surfaces.json and copy_to_blob.py)\n",
    "\n",
    "# _SKY_MTX = upload_file_to_container(\n",
    "#     blob_client, \n",
    "#     _JOB_NAME, \n",
    "#     os.path.join(_JOB_DIRECTORY, \"sky_mtx.json\")\n",
    "# )\n",
    "# _SURFACES = upload_file_to_container(\n",
    "#     blob_client, \n",
    "#     _JOB_NAME, \n",
    "#     os.path.join(_JOB_DIRECTORY, \"surfaces.json\")\n",
    "# )\n",
    "# _COPY_TO_BLOB = upload_file_to_container(\n",
    "#     blob_client, \n",
    "#     _JOB_NAME, \n",
    "#     _COPY_TO_BLOB_LOCAL\n",
    "# )\n",
    "\n",
    "#######################################################################################################\n",
    "# Upload the files unique to each simulation ([analysis_grid_0.json, ..., analysis_grid_n.json])\n",
    "\n",
    "# _ANALYSIS_GRIDS = []\n",
    "# _ANALYSIS_GRIDS_NAMES = []\n",
    "# for _file in directory_files(os.path.join(_JOB_DIRECTORY, \"AnalysisGrids\")):\n",
    "#     _ANALYSIS_GRIDS.append(upload_file_to_container(blob_client, _JOB_NAME, _file))\n",
    "#     _ANALYSIS_GRIDS_NAMES.append(os.path.basename(_file).replace(\".json\", \"\"))\n",
    "\n",
    "# print()\n",
    "\n",
    "#######################################################################################################\n",
    "# Generate a sas token and url capable of read/write\n",
    "\n",
    "container_sas_token = blob_client.generate_container_shared_access_signature(\n",
    "    _JOB_NAME, \n",
    "    permission=azureblob.BlobPermissions(\n",
    "        read=True, \n",
    "        add=True, \n",
    "        create=True, \n",
    "        write=True, \n",
    "        delete=False, \n",
    "        _str=None\n",
    "    ), \n",
    "    expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24)\n",
    ")\n",
    "\n",
    "container_sas_url = blob_client.make_container_url(\n",
    "    _JOB_NAME, \n",
    "    sas_token=container_sas_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se=2018-08-22T13%3A04%3A00Z&sp=w&sv=2017-04-17&sr=c&sig=h9K8klrifqXQUpM7q54LYTNuf7H8/qPPXSEDRyJe41Y%3D\n"
     ]
    }
   ],
   "source": [
    "# Get shared access signature providing write access to the container\n",
    "def get_container_sas_token(block_blob_client, container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the container.\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    container_sas_token = block_blob_client.generate_container_shared_access_signature(\n",
    "        container_name,\n",
    "        permission=blob_permissions,\n",
    "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24))\n",
    "    return container_sas_token\n",
    "\n",
    "output_container_sas_token = get_container_sas_token(blob_client, _JOB_NAME, azureblob.BlobPermissions.WRITE)\n",
    "print(output_container_sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bin/bash -c \"set -e; set -o pipefail; curl -fSsL https://bootstrap.pypa.io/get-pip.py | python;pip install azure-storage==0.32.0;sudo apt-get install docker -y && sudo apt-get install docker.io -y;sudo docker pull tgerrish/bhrad; wait\"'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startup_commands = [\n",
    "        \"curl -fSsL https://bootstrap.pypa.io/get-pip.py | python\",  # Install pip\n",
    "        \"pip install azure-storage==0.32.0\",  # Install the azure-storage module\n",
    "        \"sudo apt-get install docker -y && sudo apt-get install docker.io -y\",  # Install docker\n",
    "        \"sudo docker pull tgerrish/bhrad\"  # Pull RadHoneyWhale from docker hub\n",
    "    ]\n",
    "\"/bin/bash -c \\\"set -e; set -o pipefail; {}; wait\\\"\".format(\";\".join(startup_commands))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool [testpool]...\n",
      "Pool created ...\n"
     ]
    }
   ],
   "source": [
    "# Create the pool containing the compute nodes executing the tasks\n",
    "def select_latest_verified_vm_image_with_node_agent_sku(batch_client, publisher, offer, sku_starts_with):\n",
    "    \"\"\"\n",
    "    Select the latest verified image that Azure Batch supports given a publisher, offer and sku (starts with filter).\n",
    "    :param batch_client: The batch client to use.\n",
    "    :type batch_client: `batchserviceclient.BatchServiceClient`\n",
    "    :param str publisher: vm image publisher\n",
    "    :param str offer: vm image offer\n",
    "    :param str sku_starts_with: vm sku starts with filter\n",
    "    :rtype: tuple\n",
    "    :return: (node agent sku id to use, vm image ref to use)\n",
    "    \"\"\"\n",
    "    # get verified vm image list and node agent sku ids from service\n",
    "    node_agent_skus = batch_client.account.list_node_agent_skus()\n",
    "    # pick the latest supported sku\n",
    "    skus_to_use = [\n",
    "        (sku, image_ref) for sku in node_agent_skus for image_ref in sorted(\n",
    "            sku.verified_image_references, key=lambda item: item.sku)\n",
    "        if image_ref.publisher.lower() == publisher.lower() and\n",
    "        image_ref.offer.lower() == offer.lower() and\n",
    "        image_ref.sku.startswith(sku_starts_with)\n",
    "    ]\n",
    "    # skus are listed in reverse order, pick first for latest\n",
    "    sku_to_use, image_ref_to_use = skus_to_use[0]\n",
    "    return (sku_to_use.id, image_ref_to_use)\n",
    "\n",
    "def wrap_commands_in_shell(ostype, commands):\n",
    "    \"\"\"Wrap commands in a shell\n",
    "    :param list commands: list of commands to wrap\n",
    "    :param str ostype: OS type, linux or windows\n",
    "    :rtype: str\n",
    "    :return: a shell wrapping commands\n",
    "    \"\"\"\n",
    "    if ostype.lower() == \"linux\":\n",
    "        return \"/bin/bash -c \\\"set -e; set -o pipefail; {0:}; wait\\\"\".format(\";\".join(commands))\n",
    "    elif ostype.lower() == \"windows\":\n",
    "        return \"cmd.exe /c {0:}\".format(\"&\".join(commands))\n",
    "    else:\n",
    "        raise ValueError(\"unknown ostype: {}\".format(ostype))\n",
    "        \n",
    "def print_batch_exception(batch_exception):\n",
    "    \"\"\"\n",
    "    Prints the contents of the specified Batch exception.\n",
    "    :param batch_exception:\n",
    "    \"\"\"\n",
    "    print('-------------------------------------------')\n",
    "    print('Exception encountered:')\n",
    "    if (batch_exception.error and batch_exception.error.message and\n",
    "            batch_exception.error.message.value):\n",
    "        print(batch_exception.error.message.value)\n",
    "        if batch_exception.error.values:\n",
    "            print()\n",
    "            for mesg in batch_exception.error.values:\n",
    "                print('{}:\\t{}'.format(mesg.key, mesg.value))\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "def create_pool(batch_service_client, pool_id, resource_files, publisher, offer, sku, node_count):\n",
    "    \"\"\"\n",
    "    Creates a pool of compute nodes with the specified OS settings.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str pool_id: An ID for the new pool.\n",
    "    :param list resource_files: A collection of resource files for the pool's start task.\n",
    "    :param str publisher: Marketplace image publisher\n",
    "    :param str offer: Marketplace image offer\n",
    "    :param str sku: Marketplace image sku\n",
    "    \"\"\"\n",
    "    print('Creating pool [{0:}]...'.format(pool_id))\n",
    "    # Specify the commands for the pool's start task to be run on each node as it joins the pool.\n",
    "    commands = [\n",
    "        \"curl -fSsL https://bootstrap.pypa.io/get-pip.py | python\",  # Install pip\n",
    "        \"pip install azure-storage==0.32.0\",  # Install the azure-storage module\n",
    "        \"sudo apt-get install docker -y && sudo apt-get install docker.io -y\",  # Install docker\n",
    "        \"sudo docker pull tgerrish/bhrad\"  # Pull RadHoneyWhale from docker hub\n",
    "    ]\n",
    "    # Get the node agent SKU and image reference for the virtual machine configuration.\n",
    "    sku_to_use, image_ref_to_use = select_latest_verified_vm_image_with_node_agent_sku(\n",
    "        batch_service_client, \n",
    "        publisher, \n",
    "        offer, \n",
    "        sku\n",
    "    )\n",
    "    user = batchmodels.AutoUserSpecification(\n",
    "        scope=batchmodels.AutoUserScope.pool, \n",
    "        elevation_level=batchmodels.ElevationLevel.admin\n",
    "    )\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=image_ref_to_use,\n",
    "            node_agent_sku_id=sku_to_use),\n",
    "        vm_size=_POOL_VM_SIZE,\n",
    "        enable_auto_scale=True,\n",
    "        auto_scale_formula='pendingTaskSamplePercent =$PendingTasks.GetSamplePercent(180 * TimeInterval_Second);pendingTaskSamples = pendingTaskSamplePercent < 70 ? 1 : avg($PendingTasks.GetSample(180 * TimeInterval_Second)); $TargetDedicatedNodes = min(100, pendingTaskSamples);', \n",
    "        auto_scale_evaluation_interval=datetime.timedelta(minutes=5),\n",
    "        start_task=batch.models.StartTask(\n",
    "            command_line=wrap_commands_in_shell(\n",
    "                \"linux\",\n",
    "                commands),\n",
    "            user_identity=batchmodels.UserIdentity(auto_user=user),\n",
    "            wait_for_success=True,\n",
    "            resource_files=resource_files),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        batch_service_client.pool.add(\n",
    "            new_pool\n",
    "        )\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(\n",
    "            err\n",
    "        )\n",
    "        raise\n",
    "\n",
    "_POOL_NODE_COUNT = len(_ANALYSIS_GRIDS)\n",
    "\n",
    "pool = create_pool(batch_client, _POOL_ID, [_SKY_MTX, _SURFACES, _COPY_TO_BLOB], _NODE_OS_PUBLISHER, _NODE_OS_OFFER, _NODE_OS_SKU, _POOL_NODE_COUNT)\n",
    "\n",
    "print(\"Pool created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: AllocationState.steady\n"
     ]
    }
   ],
   "source": [
    "if batch_client.pool.exists(_POOL_ID):\n",
    "    my_pool = batch_client.pool.get(_POOL_ID)\n",
    "    print(\"Current state: {}\".format(my_pool.allocation_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wrap_commands_in_shell(ostype, commands):\n",
    "#     \"\"\"Wrap commands in a shell\n",
    "#     :param list commands: list of commands to wrap\n",
    "#     :param str ostype: OS type, linux or windows\n",
    "#     :rtype: str\n",
    "#     :return: a shell wrapping commands\n",
    "#     \"\"\"\n",
    "#     if ostype.lower() == \"linux\":\n",
    "#         return \"/bin/bash -c \\\"set -e; set -o pipefail; {}; wait\\\"\".format(\";\".join(commands))\n",
    "#     elif ostype.lower() == \"windows\":\n",
    "#         return \"cmd.exe /c {}\".format(\"&\".join(commands))\n",
    "#     else:\n",
    "#         raise ValueError(\"unknown ostype: {}\".format(ostype))\n",
    "        \n",
    "# input_files = _ANALYSIS_GRIDS\n",
    "# _JOB_ID = \"radjob\"\n",
    "\n",
    "# print(\"batch_service_client:\\n{0:}\\n\\njob_id:\\n{1:}\\n\\ninput_files:\\n{2:}\\n\\noutput_container_name:\\n{3:}\\n\\noutput_container_sas_token:\\n{4:}\\n\".format(batch_client, _JOB_ID, input_files, _JOB_NAME, output_container_sas_token))\n",
    "\n",
    "# print(\"Each task will have the following passed to it ... \\n\")\n",
    "\n",
    "# for input_file in input_files:\n",
    "#     print(\"New task ...\")\n",
    "    \n",
    "#     command = [\n",
    "#         'sudo bash',\n",
    "#         'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/batch tgerrish/bhrad python RunHoneybeeRadiance.py -sm ./batch/sky_mtx.json -s ./batch/surfaces.json -p ./batch/{0:}'.format(input_file.file_path),\n",
    "#         'python copy_to_blob.py --filepath ./batch/{0:} --blobname {0:} --storageaccount {1:} --storagecontainer {2:} --sastoken \"{3:}\"'.format(input_file.file_path.replace(\".json\", \"_result.json\"), _STORAGE_ACCOUNT_NAME, _JOB_NAME, output_container_sas_token)\n",
    "#     ]\n",
    "    \n",
    "# #     print(input_file.file_path)\n",
    "    \n",
    "# #     print()\n",
    "    \n",
    "#     print(\"\\n####################\\n\".join(command))\n",
    "    \n",
    "#     print()\n",
    "\n",
    "# wrap_commands_in_shell('linux', command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [radjob_0]...\n",
      "Adding 4 tasks to job [radjob_0]...\n",
      "Success! All tasks reached the 'Completed' state within the specified timeout period.\n"
     ]
    }
   ],
   "source": [
    "# Add jobs to the pool\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def create_job(batch_service_client, job_id, pool_id):\n",
    "    \"\"\"\n",
    "    Creates a job with the specified ID, associated with the specified pool.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID for the job.\n",
    "    :param str pool_id: The ID for the pool.\n",
    "    \"\"\"\n",
    "    print('Creating job [{0:}]...'.format(job_id))\n",
    "\n",
    "    job = batch.models.JobAddParameter(job_id, batch.models.PoolInformation(pool_id=pool_id))\n",
    "\n",
    "    try:\n",
    "        batch_service_client.job.add(job)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "def add_tasks(batch_service_client, job_id, input_files, output_container_name, output_container_sas_token):\n",
    "    \"\"\"\n",
    "    Adds a task for each input file in the collection to the specified job.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID of the job to which to add the tasks.\n",
    "    :param list input_files: A collection of input files. One task will be created for each input file.\n",
    "    :param output_container_name: The ID of an Azure Blob storage container to which the tasks will upload their results.\n",
    "    :param output_container_sas_token: A SAS token granting write access to the specified Azure Blob storage container.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n",
    "\n",
    "    user = batchmodels.UserIdentity(auto_user=batchmodels.AutoUserSpecification(elevation_level=batchmodels.ElevationLevel.admin, scope=batchmodels.AutoUserScope.task))\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for idx, input_file in enumerate(input_files):\n",
    "\n",
    "        command = [\n",
    "            'sudo bash',\n",
    "            'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/batch tgerrish/bhrad python RunHoneybeeRadiance.py -sm ./batch/sky_mtx.json -s ./batch/surfaces.json -p ./batch/{0:}'.format(input_file.file_path),\n",
    "            'python copy_to_blob.py --filepath ./batch/{0:} --blobname {0:} --storageaccount {1:} --storagecontainer {2:} --sastoken \"{3:}\"'.format(input_file.file_path.replace(\".json\", \"_result.json\"), _STORAGE_ACCOUNT_NAME, _JOB_NAME, output_container_sas_token)\n",
    "        ]\n",
    "\n",
    "        tasks.append(\n",
    "            batch.models.TaskAddParameter(\n",
    "                'Task_{0:}'.format(idx),\n",
    "                wrap_commands_in_shell('linux', command),\n",
    "                resource_files=[input_file],\n",
    "                user_identity=user\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    batch_service_client.task.add_collection(job_id, tasks)\n",
    "\n",
    "def wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n",
    "    \"\"\"\n",
    "    Returns when all tasks in the specified job reach the Completed state.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The id of the job whose tasks should be to monitored.\n",
    "    :param timedelta timeout: The duration to wait for task completion. If all\n",
    "    tasks in the specified job do not reach Completed state within this time\n",
    "    period, an exception will be raised.\n",
    "    \"\"\"\n",
    "    timeout_expiration = datetime.datetime.now() + timeout\n",
    "\n",
    "    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\".format(timeout), end='')\n",
    "\n",
    "    while datetime.datetime.now() < timeout_expiration:\n",
    "        print('.', end='')\n",
    "        sys.stdout.flush()\n",
    "        tasks = [batch_service_client.task.list(job) for job in job_id]\n",
    "\n",
    "        tasks = [item for sublist in tasks for item in sublist]\n",
    "\n",
    "        incomplete_tasks = [task for task in tasks if task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    print()\n",
    "    raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within timeout period of \" + str(timeout))\n",
    "\n",
    "jobs = []\n",
    "_JOB_ID = \"radjob\"\n",
    "\n",
    "for i, (files, names) in enumerate(zip(list(chunks(_ANALYSIS_GRIDS, 100)),list(chunks(_ANALYSIS_GRIDS_NAMES, 100)))):\n",
    "    job_id = _JOB_ID + \"_\" + str(i)\n",
    "    jobs.append(job_id)\n",
    "\n",
    "    # Create the job that will run the tasks.\n",
    "    create_job(batch_client, job_id, _POOL_ID)\n",
    "\n",
    "    # Add the tasks to the job. We need to supply a container shared access\n",
    "    # signature (SAS) token for the tasks so that they can upload their output\n",
    "    # to Azure Storage.\n",
    "    add_tasks(batch_client, job_id, files, _JOB_NAME, _STORAGE_ACCOUNT_KEY) #  output_container_sas_token)\n",
    "\n",
    "\n",
    "# Pause execution until tasks reach Completed state.\n",
    "#wait_for_tasks_to_complete(batch_client, jobs, datetime.timedelta(hours=24))\n",
    "\n",
    "#print(\"Success! All tasks reached the 'Completed' state within the specified timeout period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TaskAddCollectionParameter' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-269537e473b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mbatch_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tempjobid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTaskAddCollectionParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\azurebatch\\lib\\site-packages\\azure\\batch\\operations\\task_operations.py\u001b[0m in \u001b[0;36madd_collection\u001b[1;34m(self, job_id, value, task_add_collection_options, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Construct body\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mbody_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_serialize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_collection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TaskAddCollectionParameter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# Construct and send request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\azurebatch\\lib\\site-packages\\msrest\\serialization.py\u001b[0m in \u001b[0;36mbody\u001b[1;34m(self, data, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient_side_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_recursive_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\azurebatch\\lib\\site-packages\\msrest\\serialization.py\u001b[0m in \u001b[0;36m_recursive_validate\u001b[1;34m(attr_name, attr_type, data)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_recursive_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" content\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_type\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_validation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\azurebatch\\lib\\site-packages\\msrest\\serialization.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    218\u001b[0m                 \u001b[0mvalidation_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[0mvalidation_result\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_recursive_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalidation_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\azurebatch\\lib\\site-packages\\msrest\\serialization.py\u001b[0m in \u001b[0;36m_recursive_validate\u001b[1;34m(attr_name, attr_type, data)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mattr_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'['\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_recursive_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" content\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_type\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mattr_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TaskAddCollectionParameter' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Generate tasks\n",
    "tasks = []\n",
    "for idx, input_file in enumerate(_ANALYSIS_GRIDS):\n",
    "    container_url = \"https://{0:}.blob.core.windows.net/{1:}/?{3:}\".format(_STORAGE_ACCOUNT_NAME, _JOB_NAME, input_file.file_path.replace(\".json\", \"\"), output_container_sas_token)\n",
    "    command = [\n",
    "        'sudo bash',\n",
    "        'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/batch tgerrish/bhrad python RunHoneybeeRadiance.py -sm ./batch/sky_mtx.json -s ./batch/surfaces.json -p ./batch/{0:}'.format(input_file.file_path),\n",
    "        #'python copy_to_blob.py --filepath ./batch/{0:} --blobname {0:} --storageaccount {1:} --storagecontainer {2:} --sastoken \"{3:}\"'.format(input_file.file_path.replace(\".json\", \"_result.json\"), _STORAGE_ACCOUNT_NAME, _JOB_NAME, output_container_sas_token)\n",
    "    ]\n",
    "\n",
    "    temp_task = batch.models.TaskAddParameter(\n",
    "        'Task_{0:}'.format(idx), \n",
    "        wrap_commands_in_shell(\n",
    "            'linux', \n",
    "            command\n",
    "        ), \n",
    "        display_name=input_file.file_path, \n",
    "        container_settings=None, \n",
    "        exit_conditions=None, \n",
    "        resource_files=[input_file], \n",
    "        output_files=batch.models.OutputFile(\n",
    "            input_file.file_path.replace(\".json\", \"_result.json\"), \n",
    "            batch.models.OutputFileDestination(\n",
    "                container=batch.models.OutputFileBlobContainerDestination(\n",
    "                    container_url, \n",
    "                    path=None)\n",
    "            ), \n",
    "            batch.models.OutputFileUploadOptions(\"taskCompletion\")\n",
    "        ), \n",
    "        environment_settings=None, \n",
    "        affinity_info=None, \n",
    "        constraints=None, \n",
    "        user_identity=batch.models.UserIdentity(\n",
    "            auto_user=batch.models.AutoUserSpecification(\n",
    "                elevation_level=batch.models.ElevationLevel.admin, \n",
    "                scope=batch.models.AutoUserScope.task\n",
    "            )\n",
    "        ), \n",
    "        multi_instance_settings=None, \n",
    "        depends_on=None, \n",
    "        application_package_references=None, \n",
    "        authentication_token_settings=None\n",
    "    )\n",
    "\n",
    "    tasks.append(temp_task)\n",
    "\n",
    "# Create job\n",
    "# create_job(batch_client, \"tempjobid\", _POOL_ID)\n",
    "\n",
    "# Add tasks to job\n",
    "#batch_client.task.add_collection(\"tempjobid\", tasks)\n",
    "\n",
    "\n",
    "\n",
    "batch_client.task.add_collection(\"tempjobid\", batch.models.TaskAddCollectionParameter(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_properties': {},\n",
       " 'path': None,\n",
       " 'container_url': 'https://radfiles.blob.core.windows.net/0000000-testjob-3513/zone4?se=2018-08-22T13%3A04%3A00Z&sp=w&sv=2017-04-17&sr=c&sig=h9K8klrifqXQUpM7q54LYTNuf7H8/qPPXSEDRyJe41Y%3D'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.models.OutputFileBlobContainerDestination(container_url, path=None).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_tasks(<azure.batch.batch_service_client.BatchServiceClient object at 0x000002A3C3E56FD0>, radjob_0, [<azure.batch.models.resource_file.ResourceFile object at 0x000002A3C3EC5F98>, <azure.batch.models.resource_file.ResourceFile object at 0x000002A3C3EC5E80>, <azure.batch.models.resource_file.ResourceFile object at 0x000002A3C3EC5E48>, <azure.batch.models.resource_file.ResourceFile object at 0x000002A3C3EC5EB8>], 0000000-testjob-3513, se=2018-08-22T13%3A04%3A00Z&sp=w&sv=2017-04-17&sr=c&sig=h9K8klrifqXQUpM7q54LYTNuf7H8/qPPXSEDRyJe41Y%3D)\n",
      "\n",
      "user: {'additional_properties': {}, 'user_name': None, 'auto_user': <azure.batch.models.auto_user_specification.AutoUserSpecification object at 0x000002A3C4025160>}\n",
      "\n",
      "\n",
      "batch.models.TaskAddParameter(\n",
      "\tTask_0,\n",
      "\twrap_commands_in_shell('linux', COMMAND),\n",
      "\tdisplay_name=zone1,\n",
      "\tresource_files=[{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/0000000-testjob-3513/zone1.json?se=2018-08-22T13%3A03%3A53Z&sp=r&sv=2017-04-17&sr=b&sig=bu6ebn3EYu%2BnRX/IvQdpnSb1RsEhXE7/Ta815mKwhqs%3D', 'file_path': 'zone1.json', 'file_mode': None}],\n",
      "\tuser_identity=USER,\n",
      "\toutput_files=zone1_result.json)\n",
      "\n",
      "\n",
      "batch.models.TaskAddParameter(\n",
      "\tTask_1,\n",
      "\twrap_commands_in_shell('linux', COMMAND),\n",
      "\tdisplay_name=zone2,\n",
      "\tresource_files=[{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/0000000-testjob-3513/zone2.json?se=2018-08-22T13%3A03%3A54Z&sp=r&sv=2017-04-17&sr=b&sig=CyxrYA1Dkf%2BvvPheqzkR5x34ST%2B7W%2Bl0RpSoMyU3bGU%3D', 'file_path': 'zone2.json', 'file_mode': None}],\n",
      "\tuser_identity=USER,\n",
      "\toutput_files=zone2_result.json)\n",
      "\n",
      "\n",
      "batch.models.TaskAddParameter(\n",
      "\tTask_2,\n",
      "\twrap_commands_in_shell('linux', COMMAND),\n",
      "\tdisplay_name=zone3,\n",
      "\tresource_files=[{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/0000000-testjob-3513/zone3.json?se=2018-08-22T13%3A03%3A54Z&sp=r&sv=2017-04-17&sr=b&sig=Q/JW8sPOcZTmF5vE3SUNTEQgIZ1hRcH5MCt4h%2BwSEA4%3D', 'file_path': 'zone3.json', 'file_mode': None}],\n",
      "\tuser_identity=USER,\n",
      "\toutput_files=zone3_result.json)\n",
      "\n",
      "\n",
      "batch.models.TaskAddParameter(\n",
      "\tTask_3,\n",
      "\twrap_commands_in_shell('linux', COMMAND),\n",
      "\tdisplay_name=zone4,\n",
      "\tresource_files=[{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/0000000-testjob-3513/zone4.json?se=2018-08-22T13%3A03%3A54Z&sp=r&sv=2017-04-17&sr=b&sig=8CdB56jxi/rF6vMLfawmGFyZp1iR/vDPHsge%2B8WWtxU%3D', 'file_path': 'zone4.json', 'file_mode': None}],\n",
      "\tuser_identity=USER,\n",
      "\toutput_files=zone4_result.json)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"add_tasks({0:}, {1:}, {2:}, {3:}, {4:})\".format(batch_client, job_id, _ANALYSIS_GRIDS, _JOB_NAME, output_container_sas_token))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"user: {0:}\\n\\n\".format(batchmodels.UserIdentity(auto_user=batchmodels.AutoUserSpecification(elevation_level=batchmodels.ElevationLevel.admin, scope=batchmodels.AutoUserScope.task))))\n",
    "\n",
    "for idx, input_file in enumerate(_ANALYSIS_GRIDS):\n",
    "    print(\"batch.models.TaskAddParameter(\\n\\t{0:},\\n\\twrap_commands_in_shell('linux', COMMAND),\\n\\tdisplay_name={2:},\\n\\tresource_files=[{1:}],\\n\\tuser_identity=USER,\\n\\toutput_files={3:})\\n\\n\".format('Task_{0:}'.format(idx), input_file, input_file.file_path.replace(\".json\", \"\"), batchmodels.OutputFile()input_file.file_path.replace(\".json\", \"_result.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tasks(batch_service_client, job_id, input_files, output_container_name, output_container_sas_token):\n",
    "    \"\"\"\n",
    "    Adds a task for each input file in the collection to the specified job.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID of the job to which to add the tasks.\n",
    "    :param list input_files: A collection of input files. One task will be created for each input file.\n",
    "    :param output_container_name: The ID of an Azure Blob storage container to which the tasks will upload their results.\n",
    "    :param output_container_sas_token: A SAS token granting write access to the specified Azure Blob storage container.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n",
    "\n",
    "    user = batchmodels.UserIdentity(auto_user=batchmodels.AutoUserSpecification(elevation_level=batchmodels.ElevationLevel.admin, scope=batchmodels.AutoUserScope.task))\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for idx, input_file in enumerate(input_files):\n",
    "\n",
    "        command = [\n",
    "            'sudo bash',\n",
    "            'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/batch tgerrish/bhrad python RunHoneybeeRadiance.py -sm ./batch/sky_mtx.json -s ./batch/surfaces.json -p ./batch/{0:}'.format(input_file.file_path),\n",
    "            'python copy_to_blob.py --filepath ./batch/{0:} --blobname {0:} --storageaccount {1:} --storagecontainer {2:} --sastoken \"{3:}\"'.format(input_file.file_path.replace(\".json\", \"_result.json\"), _STORAGE_ACCOUNT_NAME, _JOB_NAME, output_container_sas_token)\n",
    "        ]\n",
    "\n",
    "        tasks.append(\n",
    "            batch.models.TaskAddParameter(\n",
    "                'Task_{0:}'.format(idx),\n",
    "                wrap_commands_in_shell('linux', command),\n",
    "                resource_files=[input_file],\n",
    "                user_identity=user\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    batch_service_client.task.add_collection(job_id, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next_link': '',\n",
       " 'current_page': [],\n",
       " '_current_page_iter_index': 0,\n",
       " '_derserializer': <msrest.serialization.Deserializer at 0x2a3c4011208>,\n",
       " '_get_next': <function azure.batch.operations.task_operations.TaskOperations.list.<locals>.internal_paging(next_link=None, raw=False)>,\n",
       " '_response': None,\n",
       " '_raw_headers': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = [batch_client.task.list(job) for job in job_id]\n",
    "# tasks = [item for sublist in tasks for item in sublist]\n",
    "tasks[5].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zone1', 'zone2', 'zone3', 'zone4']\n"
     ]
    }
   ],
   "source": [
    "for i, (files, names) in enumerate(zip(list(chunks(_ANALYSIS_GRIDS, 100)),list(chunks(_ANALYSIS_GRIDS_NAMES, 100)))):\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KILL ALL PROCESSES! - TIDY UP AND NEVER LOOK BACK!\n",
    "\n",
    "# Delete container\n",
    "blob_client.delete_container(_JOB_NAME)\n",
    "print(\"Container [{0:}] deleted\".format(_JOB_NAME))\n",
    "\n",
    "# Delete pool\n",
    "batch_client.pool.delete(_POOL_ID)\n",
    "print(\"Pool [{0:}] deleted\".format(_POOL_ID))\n",
    "\n",
    "# Delete job/s\n",
    "for _JOB_ID in jobs:\n",
    "    batch_client.job.delete(_JOB_ID)\n",
    "    print(\"Job [{0:}] deleted\".format(_JOB_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "_results_files = [\n",
    "    r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\docker\\azbatchworkingdir\\zone4_result.json\",\n",
    "    r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\docker\\azbatchworkingdir\\zone1_result.json\",\n",
    "    r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\docker\\azbatchworkingdir\\zone2_result.json\",\n",
    "    r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\docker\\azbatchworkingdir\\zone3_result.json\"\n",
    "]\n",
    "\n",
    "_results = []\n",
    "for _file in _results_files:\n",
    "    with open(_file) as f:\n",
    "        _results.append(json.load(f))\n",
    "\n",
    "# Combine into one set of data\n",
    "xs = [i[\"x\"] for i in _results]\n",
    "ys = [i[\"y\"] for i in _results]\n",
    "vals = [i[\"df\"] for i in _results]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.scatter(xs, ys, c=vals, cmap=\"viridis\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
