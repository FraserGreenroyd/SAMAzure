{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/Azure/azure-sdk-for-python/blob/master/doc/batch.rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import azure.batch.models as batchmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_commands_in_shell(ostype, commands):\n",
    "    \"\"\"\n",
    "    Wrap commands in a shell\n",
    "\n",
    "    :param list commands: list of commands to wrap\n",
    "    :param str ostype: OS type, linux or windows\n",
    "    :rtype: str\n",
    "    :return: a shell wrapping commands\n",
    "    \"\"\"\n",
    "    if ostype.lower() == \"linux\":\n",
    "        return \"/bin/bash -c \\\"set -e; set -o pipefail; {0:}; wait\\\"\".format(\";\".join(commands))\n",
    "    elif ostype.lower() == \"windows\":\n",
    "        return \"cmd.exe /c {0:}\".format(\"&\".join(commands))\n",
    "    else:\n",
    "        raise ValueError(\"unknown ostype: {}\".format(ostype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_yes_no(question, default=\"yes\"):\n",
    "    \"\"\"\n",
    "    Prompts the user for yes/no input, displaying the specified question text.\n",
    "\n",
    "    :param str question: The text of the prompt for input.\n",
    "    :param str default: The default if the user hits <ENTER>. Acceptable values are 'yes', 'no', and None.\n",
    "    :rtype: str\n",
    "    :return: 'yes' or 'no'\n",
    "    \"\"\"\n",
    "    valid = {'y': 'yes', 'n': 'no'}\n",
    "    if default is None:\n",
    "        prompt = ' [y/n] '\n",
    "    elif default == 'yes':\n",
    "        prompt = ' [Y/n] '\n",
    "    elif default == 'no':\n",
    "        prompt = ' [y/N] '\n",
    "    else:\n",
    "        raise ValueError(\"Invalid default answer: '{}'\".format(default))\n",
    "\n",
    "    while 1:\n",
    "        choice = input(question + prompt).lower()\n",
    "        if default and not choice:\n",
    "            return default\n",
    "        try:\n",
    "            return valid[choice[0]]\n",
    "        except (KeyError, IndexError):\n",
    "            print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_exception(batch_exception):\n",
    "    \"\"\"\n",
    "    Prints the contents of the specified Batch exception.\n",
    "\n",
    "    :param batch_exception:\n",
    "    \"\"\"\n",
    "    print('-------------------------------------------')\n",
    "    print('Exception encountered:')\n",
    "    if batch_exception.error and batch_exception.error.message and batch_exception.error.message.value:\n",
    "        print(batch_exception.error.message.value)\n",
    "        if batch_exception.error.values:\n",
    "            print()\n",
    "            for mesg in batch_exception.error.values:\n",
    "                print('{}:\\t{}'.format(mesg.key, mesg.value))\n",
    "    print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "    \"\"\"\n",
    "    Uploads a local file to an Azure Blob storage container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param str file_path: The local path to the file.\n",
    "    :rtype: `azure.batch.models.ResourceFile`\n",
    "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch\n",
    "    tasks.\n",
    "    \"\"\"\n",
    "    blob_name = os.path.basename(file_path)\n",
    "\n",
    "    print('Uploading file {} to [{}]...'.format(file_path, container_name))\n",
    "\n",
    "    block_blob_client.create_blob_from_path(container_name, blob_name, file_path)\n",
    "\n",
    "    # Obtain the SAS token for the container.\n",
    "    sas_token = get_container_sas_token(block_blob_client, container_name, azureblob.BlobPermissions.READ)\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(container_name, blob_name, sas_token=sas_token)\n",
    "\n",
    "    return batchmodels.ResourceFile(file_path=blob_name, blob_source=sas_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_container_sas_token(block_blob_client, container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the container.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container, setting the expiry time and permissions. In this case, no start time is specified, so the shared access signature becomes valid immediately. Expiration is in 2 hours.\n",
    "    container_sas_token = block_blob_client.generate_container_shared_access_signature(\n",
    "        container_name,\n",
    "        permission=blob_permissions,\n",
    "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=2)\n",
    "    )\n",
    "\n",
    "    return container_sas_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_container_sas_url(block_blob_client, container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature URL that provides write access to the ouput container to which the tasks will upload their output.\n",
    "\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS URL granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container.\n",
    "    sas_token = get_container_sas_token(block_blob_client, container_name, azureblob.BlobPermissions.WRITE)\n",
    "\n",
    "    # Construct SAS URL for the container\n",
    "    container_sas_url = \"https://{}.blob.core.windows.net/{}?{}\".format(_STORAGE_ACCOUNT_NAME, container_name,\n",
    "                                                                        sas_token)\n",
    "\n",
    "    return container_sas_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n",
    "    \"\"\"\n",
    "    Returns when all tasks in the specified job reach the Completed state.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The id of the job whose tasks should be monitored.\n",
    "    :param timedelta timeout: The duration to wait for task completion. If all tasks in the specified job do not reach Completed state within this time period, an exception will be raised.\n",
    "    \"\"\"\n",
    "    timeout_expiration = datetime.datetime.now() + timeout\n",
    "\n",
    "    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\".format(timeout), end='')\n",
    "\n",
    "    while datetime.datetime.now() < timeout_expiration:\n",
    "        print('.', end='')\n",
    "        sys.stdout.flush()\n",
    "        tasks = batch_service_client.task.list(job_id)\n",
    "        incomplete_tasks = [task for task in tasks if task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    print()\n",
    "    raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within timeout period of \" + str(timeout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL\n",
    "_BATCH_ACCOUNT_NAME = \"climatebasedbatch\"\n",
    "_BATCH_ACCOUNT_KEY = \"W94ukoxG2neFkk6teOVZ3IQ8IQjmPJqPcFq48I9lLzCrPEQSRFS/+euaUEkkSyPoulUgnx5IEZxztA9574Hluw==\"\n",
    "_BATCH_ACCOUNT_URL = \"https://climatebasedbatch.westeurope.batch.azure.com\"\n",
    "\n",
    "_STORAGE_ACCOUNT_NAME = \"radfiles\"\n",
    "_STORAGE_ACCOUNT_KEY = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "\n",
    "_POOL_ID = 'radbatchpool'\n",
    "_DEDICATED_POOL_NODE_COUNT = 0\n",
    "_LOW_PRIORITY_POOL_NODE_COUNT = 2 # TODO - Add autosize function - prevent having to manually specify number of nodes to spin up!\n",
    "_POOL_VM_SIZE = 'STANDARD_A1_v2'\n",
    "_JOB_ID = 'radbatchjob'\n",
    "\n",
    "_DIRECTORY_TO_RUN = \"./radfiles\"\n",
    "_LB_HB_FILE = \"./docker/lb_hb.tar.gz\"\n",
    "_RADIANCE_FILE = \"./docker/radiance-5.1.0-Linux.tar.gz\"\n",
    "_COPY_TO_BLOB_FILE = \"./docker/copy_to_blob.py\"\n",
    "_RUN_PROCESS_FILE = \"./docker/RunHoneybeeRadiance.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample start: 2018-08-28 14:44:01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get start time to log length of time elapsed throughout process\n",
    "start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "print('Sample start: {}'.format(start_time))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tgerrish\\\\Documents\\\\GitHub\\\\SAMAzure\\\\TestFiles\\\\Azure\\\\docker\\\\RunHoneybeeRadiance.py'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the files necessary to run the rpcoess into a container in the blob\n",
    "_LB_HB_FILEPATH = os.path.abspath(_LB_HB_FILE)\n",
    "_RADIANCE_FILEPATH = os.path.abspath(_RADIANCE_FILE)\n",
    "_COPY_TO_BLOB_FILEPATH = os.path.abspath(_COPY_TO_BLOB_FILE)\n",
    "_RUN_PROCESS_FILEPATH = os.path.abspath(_RUN_PROCESS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surfaces file:\n",
      "C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\surfaces.json\n",
      "\n",
      "Sky matrix file:\n",
      "C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\sky_mtx.json\n",
      "\n",
      "Analysis grid files:\n",
      "C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\AnalysisGrids\\zone1.json\n",
      "C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\AnalysisGrids\\zone2.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the analysis grids to process, and the associated context shading and sky matrix\n",
    "_SURFACES_FILEPATH = os.path.abspath(os.path.join(_DIRECTORY_TO_RUN, \"surfaces.json\"))\n",
    "print(\"Surfaces file:\\n{0:}\\n\".format(_SURFACES_FILEPATH))\n",
    "\n",
    "_SKY_MTX_FILEPATH = os.path.abspath(os.path.join(_DIRECTORY_TO_RUN, \"sky_mtx.json\"))\n",
    "print(\"Sky matrix file:\\n{0:}\\n\".format(_SKY_MTX_FILEPATH))\n",
    "\n",
    "_ANALYSIS_GRIDS_FILEPATHS = sorted([os.path.abspath(os.path.join(_DIRECTORY_TO_RUN, \"AnalysisGrids\", file)) for file in os.listdir(os.path.join(_DIRECTORY_TO_RUN, \"AnalysisGrids\")) if file.endswith(\".json\")])\n",
    "print(\"Analysis grid files:\")\n",
    "[print(\"{0:}\".format(i)) for i in _ANALYSIS_GRIDS_FILEPATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azure.storage.blob.blockblobservice.BlockBlobService object at 0x00000270B1410A90>\n"
     ]
    }
   ],
   "source": [
    "# Create the blob client, for use in obtaining references to blob storage containers and uploading files to containers.\n",
    "blob_client = azureblob.BlockBlobService(account_name=_STORAGE_ACCOUNT_NAME, account_key=_STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "print(blob_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container [input] created.\n",
      "Container [process] created.\n",
      "Container [output] created.\n"
     ]
    }
   ],
   "source": [
    "# Use the blob client to create the containers in Azure Storage if they don't yet exist.\n",
    "input_container_name = 'input'\n",
    "process_container_name = \"process\"\n",
    "output_container_name = 'output'\n",
    "blob_client.create_container(input_container_name, fail_on_exist=False)\n",
    "blob_client.create_container(process_container_name, fail_on_exist=False)\n",
    "blob_client.create_container(output_container_name, fail_on_exist=False)\n",
    "\n",
    "print('Container [{}] created.'.format(input_container_name))\n",
    "print('Container [{}] created.'.format(process_container_name))\n",
    "print('Container [{}] created.'.format(output_container_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\docker\\lb_hb.tar.gz to [process]...\n",
      "Uploading file C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\docker\\RunHoneybeeRadiance.py to [process]...\n"
     ]
    }
   ],
   "source": [
    "# Upload process files for processing the input files into the process directory\n",
    "lb_hb_file = upload_file_to_container(blob_client, process_container_name, _LB_HB_FILEPATH)\n",
    "run_process_file = upload_file_to_container(blob_client, process_container_name, _RUN_PROCESS_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\AnalysisGrids\\zone1.json to [input]...\n",
      "Uploading file C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\AnalysisGrids\\zone2.json to [input]...\n",
      "Uploading file C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\surfaces.json to [input]...\n",
      "Uploading file C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\sky_mtx.json to [input]...\n",
      "\n",
      "Surfaces blob:\n",
      "{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/input/surfaces.json?se=2018-08-28T15%3A52%3A50Z&sp=r&sv=2017-04-17&sr=c&sig=6S3wOYwwfQfbsIb3QlCdV38gMVuR4J%2BTKWqtW%2BGcOO8%3D', 'file_path': 'surfaces.json', 'file_mode': None}\n",
      "\n",
      "Sky matrix blob:\n",
      "{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/input/sky_mtx.json?se=2018-08-28T15%3A53%3A01Z&sp=r&sv=2017-04-17&sr=c&sig=Ok2e3eW4akEIkp%2B7kdobO3okwHTn76kZdBpNkEU5fd4%3D', 'file_path': 'sky_mtx.json', 'file_mode': None}\n",
      "\n",
      "Analysis grid blobs:\n",
      "{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/input/zone1.json?se=2018-08-28T15%3A52%3A49Z&sp=r&sv=2017-04-17&sr=c&sig=TF%2BGn6RI7oZ4qxg3NMyNitrf3AARcCn4YBKlV61EkS8%3D', 'file_path': 'zone1.json', 'file_mode': None}\n",
      "{'additional_properties': {}, 'blob_source': 'https://radfiles.blob.core.windows.net/input/zone2.json?se=2018-08-28T15%3A52%3A50Z&sp=r&sv=2017-04-17&sr=c&sig=6S3wOYwwfQfbsIb3QlCdV38gMVuR4J%2BTKWqtW%2BGcOO8%3D', 'file_path': 'zone2.json', 'file_mode': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload files for processing into the input directory\n",
    "analysis_grid_files = [upload_file_to_container(blob_client, input_container_name, file_path) for file_path in _ANALYSIS_GRIDS_FILEPATHS]\n",
    "surfaces_file = upload_file_to_container(blob_client, input_container_name, _SURFACES_FILEPATH)\n",
    "sky_mtx_file = upload_file_to_container(blob_client, input_container_name, _SKY_MTX_FILEPATH)\n",
    "\n",
    "print(\"\\nSurfaces blob:\\n{0:}\\n\".format(surfaces_file))\n",
    "print(\"Sky matrix blob:\\n{0:}\\n\".format(sky_mtx_file))\n",
    "print(\"Analysis grid blobs:\")\n",
    "[print(\"{0:}\".format(i)) for i in analysis_grid_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://radfiles.blob.core.windows.net/output?se=2018-08-28T15%3A53%3A09Z&sp=w&sv=2017-04-17&sr=c&sig=XBiXEaj2plorzGbUlFqfUddT34FoUUrl15VHmQW/P3w%3D\n"
     ]
    }
   ],
   "source": [
    "# Obtain a shared access signature URL that provides write access to the output container to which the tasks will upload their output.\n",
    "output_container_sas_url = get_container_sas_url(blob_client, output_container_name, azureblob.BlobPermissions.WRITE)\n",
    "print(output_container_sas_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azure.batch.batch_auth.SharedKeyCredentials object at 0x00000270B14F9C88>\n",
      "<azure.batch.batch_service_client.BatchServiceClient object at 0x00000270B14F95C0>\n"
     ]
    }
   ],
   "source": [
    "# Create a Batch service client. We'll now be interacting with the Batch service in addition to Storage\n",
    "credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME, _BATCH_ACCOUNT_KEY)\n",
    "batch_client = batch.BatchServiceClient(credentials, base_url=_BATCH_ACCOUNT_URL)\n",
    "\n",
    "print(credentials)\n",
    "print(batch_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RERUN FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [radbatchjob]...\n"
     ]
    },
    {
     "ename": "BatchErrorException",
     "evalue": "{'additional_properties': {}, 'lang': 'en-US', 'value': 'The specified job already exists.\\nRequestId:52541cd3-b0db-43a6-bfa2-16de914a64f8\\nTime:2018-08-28T14:57:56.2434901Z'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBatchErrorException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-9857c204ad9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the job to which tasks will be assigned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Creating job [{}]...'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_JOB_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbatch_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJobAddParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_JOB_ID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPoolInformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_POOL_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda2\\envs\\azurebatch\\lib\\site-packages\\azure\\batch\\operations\\job_operations.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, job, job_add_options, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m201\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchErrorException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBatchErrorException\u001b[0m: {'additional_properties': {}, 'lang': 'en-US', 'value': 'The specified job already exists.\\nRequestId:52541cd3-b0db-43a6-bfa2-16de914a64f8\\nTime:2018-08-28T14:57:56.2434901Z'}"
     ]
    }
   ],
   "source": [
    "# Create the job to which tasks will be assigned\n",
    "print('Creating job [{}]...'.format(_JOB_ID))\n",
    "batch_client.job.add(batch.models.JobAddParameter(_JOB_ID, batch.models.PoolInformation(pool_id=_POOL_ID)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tasks to the job\n",
    "\n",
    "print('Adding {} tasks to job [{}]...'.format(len(analysis_grid_files), _JOB_ID))\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for idx, analysis_grid_file in enumerate(analysis_grid_files):\n",
    "    grid_file_path = analysis_grid_file.file_path\n",
    "    sky_mtx_file_path = sky_mtx_file.file_path\n",
    "    surfaces_file_path = surfaces_file.file_path\n",
    "    results_file_path = grid_file_path.replace(\".json\", \"_result.json\")\n",
    "    \n",
    "    # print(grid_file_path, sky_mtx_file_path, surfaces_file_path, results_file_path)\n",
    "    \n",
    "    commands = [\n",
    "#         \"docker run --name abc -t -d tgerrish/bhrad bash\",\n",
    "        \"apt-get update\",\n",
    "        \"apt-get install wget\",\n",
    "        \"apt-get install rsync\",\n",
    "        \"wget https://github.com/NREL/Radiance/releases/download/5.1.0/radiance-5.1.0-Linux.tar.gz\",\n",
    "        \"tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "        \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "        \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "        \"tar xzf lb_hb.tar.gz\",\n",
    "        \"python3 RunHoneybeeRadiance.py -sm {0:} -s {1:} -p {2:}\".format(sky_mtx_file_path, surfaces_file_path, grid_file_path),\n",
    "#         \"docker run --name abc -v /var/run/docker.sock:/var/run/docker.sock -t -d tgerrish/bhrad bash\",\n",
    "#         \"docker cp ./{0:} abc:/surfaces.json\".format(surfaces_file_path),\n",
    "#         \"docker cp ./{0:} abc:/sky_mtx.json\".format(sky_mtx_file_path),\n",
    "#         \"docker cp ./{0:} abc:/{0:}\".format(grid_file_path),\n",
    "#         \"docker exec abc python RunHoneybeeRadiance.py -sm {0:} -s {1:} -p {2:}\".format(sky_mtx_file_path, surfaces_file_path, grid_file_path),\n",
    "#         \"docker cp abc:./{0:} ./{0:}\".format(results_file_path),\n",
    "#         \"docker stop abc\", \n",
    "#         \"docker rm abc\"\n",
    "    ]\n",
    "    \n",
    "    # print(\"\\n{0:}\\n\".format(\"\\n\".join(commands)))\n",
    "    \n",
    "    command = wrap_commands_in_shell(\"linux\", commands)\n",
    "    \n",
    "    # print(command)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    tasks.append(\n",
    "        batch.models.TaskAddParameter(\n",
    "            id='task_{0:}'.format(re.sub(\"[^0-9a-zA-Z]\", \"\", grid_file_path.replace(\".json\", \"\"))), \n",
    "                command_line=command, \n",
    "                resource_files=[\n",
    "                    analysis_grid_file, \n",
    "                    sky_mtx_file, \n",
    "                    surfaces_file,\n",
    "                    lb_hb_file,\n",
    "                    run_process_file,\n",
    "                ],\n",
    "                output_files=[\n",
    "                    batchmodels.OutputFile(\n",
    "                        results_file_path, \n",
    "                        destination=batchmodels.OutputFileDestination(\n",
    "                            container=batchmodels.OutputFileBlobContainerDestination(\n",
    "                                output_container_sas_url\n",
    "                            )\n",
    "                        ),\n",
    "                        upload_options=batchmodels.OutputFileUploadOptions(\n",
    "                            batchmodels.OutputFileUploadCondition.task_success\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "batch_tasks = batch_client.task.add_collection(_JOB_ID, tasks)\n",
    "\n",
    "print(batch_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool [radbatchpool]...\n",
      "{'additional_properties': {}, 'id': 'radbatchpool', 'display_name': None, 'vm_size': 'STANDARD_A1_v2', 'cloud_service_configuration': None, 'virtual_machine_configuration': <azure.batch.models.virtual_machine_configuration.VirtualMachineConfiguration object at 0x00000270B160C320>, 'resize_timeout': None, 'target_dedicated_nodes': 0, 'target_low_priority_nodes': 2, 'enable_auto_scale': None, 'auto_scale_formula': None, 'auto_scale_evaluation_interval': None, 'enable_inter_node_communication': None, 'network_configuration': None, 'start_task': <azure.batch.models.start_task.StartTask object at 0x00000270B160C470>, 'certificate_references': None, 'application_package_references': None, 'application_licenses': None, 'max_tasks_per_node': None, 'task_scheduling_policy': None, 'user_accounts': None, 'metadata': None}\n"
     ]
    }
   ],
   "source": [
    "# Create the pool that will contain the compute nodes that will execute the tasks.\n",
    "\n",
    "print('Creating pool [{}]...'.format(_POOL_ID))\n",
    "\n",
    "commands = [\n",
    "#     \"apt-get update\",\n",
    "#     \"apt-get install wget\",\n",
    "#     \"apt-get install rsync\",\n",
    "#     \"wget https://github.com/NREL/Radiance/releases/download/5.1.0/radiance-5.1.0-Linux.tar.gz\",\n",
    "#     \"tar xzf radiance-5.1.0-Linux.tar.gz\",\n",
    "#     \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/bin/ /usr/local/bin/\",\n",
    "#     \"rsync -av /radiance-5.1.0-Linux/usr/local/radiance/lib/ /usr/local/lib/ray/\",\n",
    "#     \"apt-get install docker -y\",\n",
    "#     \"apt-get install docker.io -y\",\n",
    "#     \"docker pull tgerrish/bhrad\"\n",
    "    \"ls\",\n",
    "    \"ls\"\n",
    "]\n",
    "\n",
    "command = wrap_commands_in_shell(\"linux\", commands)\n",
    "\n",
    "new_pool = batch.models.PoolAddParameter(\n",
    "    id=_POOL_ID,\n",
    "    virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "        image_reference=batchmodels.ImageReference(\n",
    "            publisher=\"Canonical\", \n",
    "            offer=\"UbuntuServer\", \n",
    "            sku=\"16.04-LTS\",\n",
    "            version=\"latest\"\n",
    "        ),\n",
    "        node_agent_sku_id=\"batch.node.ubuntu 16.04\"\n",
    "    ),\n",
    "    vm_size=_POOL_VM_SIZE,\n",
    "    target_dedicated_nodes=_DEDICATED_POOL_NODE_COUNT,\n",
    "    target_low_priority_nodes=_LOW_PRIORITY_POOL_NODE_COUNT,\n",
    "    start_task=batchmodels.StartTask(\n",
    "        command_line=command, \n",
    "        wait_for_success=True,\n",
    "        user_identity=batchmodels.UserIdentity(\n",
    "            auto_user=batchmodels.AutoUserSpecification(\n",
    "                scope=batchmodels.AutoUserScope.pool,\n",
    "                elevation_level=batchmodels.ElevationLevel.admin\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(new_pool)\n",
    "\n",
    "batch_client.pool.add(new_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring all tasks for 'Completed' state, timeout in 0:30:00.........................................\n",
      "Success! All tasks reached the 'Completed' state within the specified timeout period.\n"
     ]
    }
   ],
   "source": [
    "wait_for_tasks_to_complete(batch_client, _JOB_ID, datetime.timedelta(minutes=30))\n",
    "print(\"Success! All tasks reached the 'Completed' state within the specified timeout period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear tasks from job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete job\n",
    "batch_client.job.delete(_JOB_ID)\n",
    "print(\"Job [{0:}] deleted\".format(_JOB_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool [radbatchpool] deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete pool\n",
    "batch_client.pool.delete(_POOL_ID)\n",
    "print(\"Pool [{0:}] deleted\".format(_POOL_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
