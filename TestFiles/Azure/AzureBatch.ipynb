{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global keys\n",
    "\n",
    "_BATCH_ACCOUNT_NAME = \"climatebasedbatch\"\n",
    "_BATCH_ACCOUNT_KEY = \"W94ukoxG2neFkk6teOVZ3IQ8IQjmPJqPcFq48I9lLzCrPEQSRFS/+euaUEkkSyPoulUgnx5IEZxztA9574Hluw==\"\n",
    "_BATCH_ACCOUNT_URL = \"https://climatebasedbatch.westeurope.batch.azure.com\"\n",
    "\n",
    "_STORAGE_ACCOUNT_NAME = \"radfiles\"\n",
    "_STORAGE_ACCOUNT_KEY = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "\n",
    "_POOL_ID = \"newpool\"  # \"1st_deployment\"\n",
    "_MIN_POOL_NODE = 1\n",
    "_MAX_POOL_NODE = 100\n",
    "\n",
    "_POOL_VM_SIZE = 'BASIC_A1'\n",
    "_NODE_OS_PUBLISHER = 'Canonical'\n",
    "_NODE_OS_OFFER = 'UbuntuServer'\n",
    "_NODE_OS_SKU = '16'\n",
    "\n",
    "_JOB_NAME = \"0000000-jobname-3513\"\n",
    "\n",
    "# _RADIATION = \"honeybee_scripts/run_radiation.py\"\n",
    "# _DAYLIGHT_FACTOR = \"honeybee_scripts/run_daylight_factor.py\"\n",
    "# _ANNUAL = \"honeybee_scripts/run_annual.py\"\n",
    "# _DAYLIGHT_COEFF = \"honeybee_scripts/run_daylight_coeff.py\"\n",
    "# _COPY_TO_OUTPUT = 'copyToBlob.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.models as batchmodels\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob client generated:  <azure.storage.blob.blockblobservice.BlockBlobService object at 0x000001E3651E7550>\n",
      "Credentials generated:  <azure.batch.batch_auth.SharedKeyCredentials object at 0x000001E3651E7710>\n",
      "Batch client generated: <azure.batch.batch_service_client.BatchServiceClient object at 0x000001E3651E75F8>\n"
     ]
    }
   ],
   "source": [
    "# Create the blob client - for use in obtaining referecnes to the blob stroage containers and uploading files to those containers\n",
    "blob_client = azureblob.BlockBlobService(account_name=_STORAGE_ACCOUNT_NAME, account_key=_STORAGE_ACCOUNT_KEY)\n",
    "print(\"Blob client generated:  {0:}\".format(blob_client))\n",
    "\n",
    "# Generate shard key credentials enabling transaction with the batch client\n",
    "credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME, _BATCH_ACCOUNT_KEY)\n",
    "print(\"Credentials generated:  {0:}\".format(credentials))\n",
    "\n",
    "# Create the batch client - for use in transacting with batch nodes\n",
    "batch_client = batch.BatchServiceClient(credentials, base_url=_BATCH_ACCOUNT_URL)\n",
    "print(\"Batch client generated: {0:}\".format(batch_client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container created: [0000000-jobname-3513]\n"
     ]
    }
   ],
   "source": [
    "# Create container for job file storage\n",
    "blob_client.create_container(_JOB_NAME, fail_on_exist=False)\n",
    "print(\"Container created: [{0:}]\".format(_JOB_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file surfaces.json to container [0000000-jobname-3513/surfaces.json]\n",
      "Uploading file sky_mtx.json to container [0000000-jobname-3513/sky_mtx.json]\n",
      "[<azure.batch.models.resource_file.ResourceFile object at 0x000002B6275DA978>, <azure.batch.models.resource_file.ResourceFile object at 0x000002B6275DA5F8>]\n",
      "Uploading file zone1.json to container [0000000-jobname-3513/zone1.json]\n",
      "Uploading file zone2.json to container [0000000-jobname-3513/zone2.json]\n",
      "Uploading file zone3.json to container [0000000-jobname-3513/zone3.json]\n",
      "Uploading file zone4.json to container [0000000-jobname-3513/zone4.json]\n",
      "[<azure.batch.models.resource_file.ResourceFile object at 0x000002B6275DA860>, <azure.batch.models.resource_file.ResourceFile object at 0x000002B6275DA780>, <azure.batch.models.resource_file.ResourceFile object at 0x000002B6275DA748>, <azure.batch.models.resource_file.ResourceFile object at 0x000002B6275DA7B8>]\n"
     ]
    }
   ],
   "source": [
    "# Upload sky matrix and surfaces files into job container\n",
    "\n",
    "def directory_files(directory):\n",
    "    \"\"\"\n",
    "    Generates a list of the files within a directory\n",
    "    :param str directory: A path to a directory.\n",
    "    :return list: List of files\n",
    "    \"\"\"\n",
    "    return [item for sublist in [[os.path.join(path, name) for name in files] for path, subdirs, files in os.walk(directory)] for item in sublist]\n",
    "\n",
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "    \"\"\"\n",
    "    Uploads a local file to an Azure Blob storage container.\n",
    "    :param block_blob_client: An Azure blockblobservice client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param str file_path: The local path to the file.\n",
    "    :rtype: `azure.batch.models.ResourceFile`\n",
    "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    blob_name = os.path.basename(file_path)\n",
    "    print('Uploading file {0:} to container [{1:}/{0:}]'.format(blob_name, container_name))\n",
    "    block_blob_client.create_blob_from_path(container_name, blob_name, file_path)\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(container_name, blob_name, permission=azureblob.BlobPermissions.READ, expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24))\n",
    "    sas_url = block_blob_client.make_blob_url(container_name, blob_name, sas_token=sas_token)\n",
    "    \n",
    "    return batchmodels.ResourceFile(file_path=blob_name, blob_source=sas_url)\n",
    "\n",
    "# Upload the files common to all simulations (Sky Matrix and Context Surfaces)\n",
    "common_file_paths = [\n",
    "    os.path.realpath(r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\surfaces.json\"),\n",
    "    os.path.realpath(r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\sky_mtx.json\")\n",
    "]\n",
    "common_file_names = [os.path.basename(i) for i in common_file_paths]\n",
    "common_files = [upload_file_to_container(blob_client, _JOB_CONTAINER_NAME, i) for i in common_file_paths]\n",
    "print(common_files)\n",
    "\n",
    "# Upload the files unique to each simulation (individual analysis grids)\n",
    "grid_file_paths = directory_files(r\"C:\\Users\\tgerrish\\Documents\\GitHub\\SAMAzure\\TestFiles\\Azure\\radfiles\\AnalysisGrids\")\n",
    "grid_file_names = [os.path.basename(i) for i in grid_file_paths]\n",
    "grid_files = [upload_file_to_container(blob_client, _JOB_CONTAINER_NAME, i) for i in grid_file_paths]\n",
    "print(grid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se=2018-08-17T15%3A19%3A46Z&sp=w&sv=2017-04-17&sr=c&sig=0kLOiZ35qtrHpoS2AhaMUPB2KjojafMESffNQ1IT1Rk%3D\n"
     ]
    }
   ],
   "source": [
    "# Get shared access signature providing write access to the container\n",
    "def get_container_sas_token(block_blob_client, container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the\n",
    "    container.\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "\n",
    "    container_sas_token = block_blob_client.generate_container_shared_access_signature(\n",
    "        container_name,\n",
    "        permission=blob_permissions,\n",
    "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24))\n",
    "\n",
    "    return container_sas_token\n",
    "\n",
    "output_container_sas_token = get_container_sas_token(blob_client, _JOB_CONTAINER_NAME, azureblob.BlobPermissions.WRITE)\n",
    "print(output_container_sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool [newpool]...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the pool containing the compute nodes executing the tasks\n",
    "\n",
    "def select_latest_verified_vm_image_with_node_agent_sku(batch_client, publisher, offer, sku_starts_with):\n",
    "    \"\"\"Select the latest verified image that Azure Batch supports given a publisher, offer and sku (starts with filter).\n",
    "    :param batch_client: The batch client to use.\n",
    "    :type batch_client: `batchserviceclient.BatchServiceClient`\n",
    "    :param str publisher: vm image publisher\n",
    "    :param str offer: vm image offer\n",
    "    :param str sku_starts_with: vm sku starts with filter\n",
    "    :rtype: tuple\n",
    "    :return: (node agent sku id to use, vm image ref to use)\n",
    "    \"\"\"\n",
    "    # get verified vm image list and node agent sku ids from service\n",
    "    node_agent_skus = batch_client.account.list_node_agent_skus()\n",
    "    # pick the latest supported sku\n",
    "    skus_to_use = [\n",
    "        (sku, image_ref) for sku in node_agent_skus for image_ref in sorted(\n",
    "            sku.verified_image_references, key=lambda item: item.sku)\n",
    "        if image_ref.publisher.lower() == publisher.lower() and\n",
    "        image_ref.offer.lower() == offer.lower() and\n",
    "        image_ref.sku.startswith(sku_starts_with)\n",
    "    ]\n",
    "    # skus are listed in reverse order, pick first for latest\n",
    "    sku_to_use, image_ref_to_use = skus_to_use[0]\n",
    "    return (sku_to_use.id, image_ref_to_use)\n",
    "\n",
    "def wrap_commands_in_shell(ostype, commands):\n",
    "    \"\"\"Wrap commands in a shell\n",
    "    :param list commands: list of commands to wrap\n",
    "    :param str ostype: OS type, linux or windows\n",
    "    :rtype: str\n",
    "    :return: a shell wrapping commands\n",
    "    \"\"\"\n",
    "    if ostype.lower() == \"linux\":\n",
    "        return \"/bin/bash -c \\\"set -e; set -o pipefail; {}; wait\\\"\".format(\";\".join(commands))\n",
    "    elif ostype.lower() == \"windows\":\n",
    "        return \"cmd.exe /c {}\".format(\"&\".join(commands))\n",
    "    else:\n",
    "        raise ValueError(\"unknown ostype: {}\".format(ostype))\n",
    "        \n",
    "def print_batch_exception(batch_exception):\n",
    "    \"\"\"\n",
    "    Prints the contents of the specified Batch exception.\n",
    "    :param batch_exception:\n",
    "    \"\"\"\n",
    "    print('-------------------------------------------')\n",
    "    print('Exception encountered:')\n",
    "    if (batch_exception.error and batch_exception.error.message and\n",
    "            batch_exception.error.message.value):\n",
    "        print(batch_exception.error.message.value)\n",
    "        if batch_exception.error.values:\n",
    "            print()\n",
    "            for mesg in batch_exception.error.values:\n",
    "                print('{}:\\t{}'.format(mesg.key, mesg.value))\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "def create_pool(batch_service_client, pool_id, resource_files, publisher, offer, sku, node_count):\n",
    "    \"\"\"\n",
    "    Creates a pool of compute nodes with the specified OS settings.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str pool_id: An ID for the new pool.\n",
    "    :param list resource_files: A collection of resource files for the pool's\n",
    "    start task.\n",
    "    :param str publisher: Marketplace image publisher\n",
    "    :param str offer: Marketplace image offer\n",
    "    :param str sku: Marketplace image sku\n",
    "    \"\"\"\n",
    "    print('Creating pool [{}]...'.format(pool_id))\n",
    "\n",
    "    # Specify the commands for the pool's start task to be run on each node as it joins the pool.\n",
    "    task_commands = [\n",
    "        # Install pip\n",
    "        \"curl -fSsL https://bootstrap.pypa.io/get-pip.py | python\",\n",
    "        # Install the azure-storage module so that the task script can access Azure Blob storage\n",
    "        \"pip install azure-storage==0.32.0\",\n",
    "        # Install docker\n",
    "        \"sudo apt-get install docker -y && sudo apt-get install docker.io -y\",\n",
    "        # Pull RadHoneyWhale from docker hub\n",
    "        \"sudo docker pull tgerrish/bhrad\"]\n",
    "\n",
    "    # Get the node agent SKU and image reference for the virtual machine configuration.\n",
    "    sku_to_use, image_ref_to_use = select_latest_verified_vm_image_with_node_agent_sku(batch_service_client, publisher, offer, sku)\n",
    "    user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=image_ref_to_use,\n",
    "            node_agent_sku_id=sku_to_use),\n",
    "        vm_size=_POOL_VM_SIZE,\n",
    "        resize_timeout=datetime.timedelta(minutes=15),\n",
    "        target_dedicated_nodes=_POOL_NODE_COUNT,\n",
    "        start_task=batch.models.StartTask(\n",
    "            command_line=wrap_commands_in_shell(\n",
    "                \"linux\",\n",
    "                task_commands),\n",
    "            user_identity=batchmodels.UserIdentity(auto_user=user),\n",
    "            wait_for_success=True,\n",
    "            resource_files=resource_files),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        batch_service_client.pool.add(new_pool)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "_POOL_NODE_COUNT = 4\n",
    "\n",
    "pool = create_pool(batch_client, _POOL_ID, [], _NODE_OS_PUBLISHER, _NODE_OS_OFFER, _NODE_OS_SKU, _POOL_NODE_COUNT)\n",
    "print(\"Pool created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [batch_0]...\n"
     ]
    }
   ],
   "source": [
    "# Add jobs to the pool\n",
    "\n",
    "_JOB_ID = \"batch\"\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def create_job(batch_service_client, job_id, pool_id):\n",
    "    \"\"\"\n",
    "    Creates a job with the specified ID, associated with the specified pool.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID for the job.\n",
    "    :param str pool_id: The ID for the pool.\n",
    "    \"\"\"\n",
    "    print('Creating job [{0:}]...'.format(job_id))\n",
    "\n",
    "    job = batch.models.JobAddParameter(job_id, batch.models.PoolInformation(pool_id=pool_id))\n",
    "\n",
    "    try:\n",
    "        batch_service_client.job.add(job)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "# Chunkify the jobs into batches of 100 (if greater than 100 grids to run)\n",
    "jobs = []\n",
    "for i, (files, names) in enumerate(zip(list(chunks(grid_files, 100)), list(chunks(grid_file_names, 100)))):\n",
    "    job_id = \"{0:}_{1:}\".format(_JOB_ID, i)\n",
    "    jobs.append(job_id)\n",
    "    \n",
    "    # Create the job that will run the task\n",
    "    create_job(batch_client, job_id, _POOL_ID)\n",
    "    \n",
    "    # Add tasks to the job\n",
    "#     add_tasks(batch_client, job_id, files, names, OUTPUT_CONTAINER_NAME, output_container_sas_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_tasks(batch_service_client, job_id, input_files,\n",
    "#               input_file_names, output_container_name, output_container_sas_token):\n",
    "#     \"\"\"\n",
    "#     Adds a task for each input file in the collection to the specified job.\n",
    "\n",
    "#     :param batch_service_client: A Batch service client.\n",
    "#     :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "#     :param str job_id: The ID of the job to which to add the tasks.\n",
    "#     :param list input_files: A collection of input files. One task will be created for each input file.\n",
    "#     :param output_container_name: The ID of an Azure Blob storage container to which the tasks will upload their results.\n",
    "#     :param output_container_sas_token: A SAS token granting write access to the specified Azure Blob storage container.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n",
    "\n",
    "#     # user = batchmodels.UserIdentity(auto_user=batchmodels.AutoUserSpecification(elevation_level=batchmodels.ElevationLevel.admin, scope=batchmodels.AutoUserScope.task))\n",
    "\n",
    "#     tasks = []\n",
    "\n",
    "#     for idx, input_file in enumerate(input_files):\n",
    "#         filename = input_file_names[idx].split(\".\")[0]\n",
    "\n",
    "#         command = [\n",
    "#             \"sudo bash\",\n",
    "#             \"sudo docker run -it -d --name temp tgerrish/bhrad:latest\",\n",
    "#             \"sudo docker cp $AZ_BATCH_TASK_WORKING_DIR/analysisGrid.json temp:/grid.json\",\n",
    "#             \"sudo docker cp $AZ_BATCH_TASK_WORKING_DIR/sky_mtx.json temp:/sky_mtx.json\",\n",
    "#             \"sudo docker cp $AZ_BATCH_TASK_WORKING_DIR/surfaces.json temp:/surfaces.json\"\n",
    "#             \"sudo docker exec temp python RunHoneybeeRadiance.py -p ./grid.json -sm ./sky_mtx.json -s ./surfaces.json\"\n",
    "#             \"sudo docker cp temp:/grid_result.json grid_result.json\"\n",
    "#             --filepath RunHoneybeeRadiance.py --blobname \"runhbrad.py\" --storageaccount \"radfiles\" --storagecontainer \"0000000-jobname-3513\" --sastoken \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "            \n",
    "#             'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job:z antoinedao/radhoneywhale unzip /usr/job/{} -d /usr/job/'.format(input_file_names[idx]),\n",
    "#                     'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale python3 /usr/convertToBash.py '\n",
    "#                     '--filepath /usr/job',\n",
    "#                     'sudo chmod +x $AZ_BATCH_TASK_WORKING_DIR/gridbased_annual/commands.sh',\n",
    "#                     'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale /usr/job/gridbased_annual/commands.sh',\n",
    "#                     'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale zip -r /usr/job/out_{} /usr/job/'.format(filename + \".zip\"),\n",
    "#                     'python $AZ_BATCH_NODE_SHARED_DIR/copyToBlob.py '\n",
    "#                     '--filepath $AZ_BATCH_TASK_WORKING_DIR/{} --filename {} --storageaccount {} '\n",
    "#                     '--storagecontainer {} --sastoken \"{}\"'.format(\n",
    "#                      'out_'+filename + \".zip\",\n",
    "#                      'out_'+filename + \".zip\",\n",
    "#                      _STORAGE_ACCOUNT_NAME,\n",
    "#                      output_container_name,\n",
    "#                      output_container_sas_token)]\n",
    "\n",
    "#         tasks.append(batch.models.TaskAddParameter(\n",
    "#                 'Task_{}'.format(idx,filename),\n",
    "#                 common.helpers.wrap_commands_in_shell('linux', command),\n",
    "#                 resource_files=[input_file],\n",
    "#                 user_identity=user\n",
    "#                 )\n",
    "#         )\n",
    "\n",
    "#     batch_service_client.task.add_collection(job_id, tasks)\n",
    "\n",
    "batch_service_client = batch_client\n",
    "job_id = _JOB_ID + \"_0\"\n",
    "input_files\n",
    "input_file_names\n",
    "output_container_name\n",
    "output_container_sas_token\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker run --name xyz bhrad:latest\n",
      "docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/AnalysisGrids/zone1.json xyz:/home/grid.json\n",
      "docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/sky_mtx.json xyz:/home/sky_mtx.json\n",
      "docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/surfaces.json xyz:/home/surfaces.json\n",
      "docker exec xyz python _RunHoneybeeRadiance.py -p ./home/grid.json -sm ./home/sky_mtx.json -s ./home/surfaces.json -o ./home\n",
      "docker cp xyz:/home/grid_result.json grid_result.json\n",
      "\n",
      "\n",
      "\n",
      "docker run --name xyz bhrad:latest \\\n",
      "&& docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/AnalysisGrids/zone1.json xyz:/home/grid.json \\\n",
      "&& docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/sky_mtx.json xyz:/home/sky_mtx.json \\\n",
      "&& docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/surfaces.json xyz:/home/surfaces.json \\\n",
      "&& docker exec xyz python _RunHoneybeeRadiance.py -p ./home/grid.json -sm ./home/sky_mtx.json -s ./home/surfaces.json -o ./home \\\n",
      "&& docker cp xyz:/home/grid_result.json grid_result.json\n"
     ]
    }
   ],
   "source": [
    "# COMMANDS FOR THE BATCH NODE\n",
    "\n",
    "DOCKERIMAGE = \"bhrad:latest\"\n",
    "IMAGENAME = \"xyz\"\n",
    "PATHTOHOSTFILE_ANALYSISGRID = \"C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/AnalysisGrids/zone1.json\"\n",
    "PATHTOHOSTFILE_SKYMATRIX = \"C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/sky_mtx.json\"\n",
    "PATHTOHOSTFILE_SURFACES = \"C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/surfaces.json\"\n",
    "\n",
    "# Start docker container\n",
    "print(\"docker run --name {0:} {1:}\".format(IMAGENAME, DOCKERIMAGE))\n",
    "\n",
    "# Copy files from host to container\n",
    "print(\"docker cp {0:} {1:}:/grid.json\".format(PATHTOHOSTFILE_ANALYSISGRID, IMAGENAME))\n",
    "print(\"docker cp {0:} {1:}:/sky_mtx.json\".format(PATHTOHOSTFILE_SKYMATRIX, IMAGENAME))\n",
    "print(\"docker cp {0:} {1:}:/surfaces.json\".format(PATHTOHOSTFILE_SURFACES, IMAGENAME))\n",
    "\n",
    "# Run Radiance command for daylight metrics processing\n",
    "print(\"docker exec {0:} python RunHoneybeeRadiance.py -p ./grid.json -sm ./sky_mtx.json -s ./surfaces.json -o ./home\".format(IMAGENAME))\n",
    "\n",
    "# Copy result file back to host\n",
    "print(\"docker cp {0:}:/home/grid_result.json grid_result.json\".format(IMAGENAME))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"docker run --name {0:} {1:} \\\\\\n&& docker cp {2:} {0:}:/home/grid.json \\\\\\n&& docker cp {3:} {0:}:/home/sky_mtx.json \\\\\\n&& docker cp {4:} {0:}:/home/surfaces.json \\\\\\n&& docker exec {0:} python _RunHoneybeeRadiance.py -p ./home/grid.json -sm ./home/sky_mtx.json -s ./home/surfaces.json -o ./home \\\\\\n&& docker cp {0:}:/home/grid_result.json grid_result.json\".format(IMAGENAME, DOCKERIMAGE, PATHTOHOSTFILE_ANALYSISGRID, PATHTOHOSTFILE_SKYMATRIX, PATHTOHOSTFILE_SURFACES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker run --name yolo bhrad:latest && sleep 2 && docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/AnalysisGrids/zone1.json yolo:/home/grid.json && docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/sky_mtx.json yolo:/home/sky_mtx.json && docker cp C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/surfaces.json yolo:/home/surfaces.json && docker exec yolo python _RunHoneybeeRadiance.py -p ./home/grid.json -sm ./home/sky_mtx.json -s ./home/surfaces.json -o ./home && docker cp yolo:/home/grid_result.json grid_result.json\n"
     ]
    }
   ],
   "source": [
    "print(\"docker run --name {0:} {1:} && sleep 2 && docker cp {2:} {0:}:/home/grid.json && docker cp {3:} {0:}:/home/sky_mtx.json && docker cp {4:} {0:}:/home/surfaces.json && docker exec {0:} python _RunHoneybeeRadiance.py -p ./home/grid.json -sm ./home/sky_mtx.json -s ./home/surfaces.json -o ./home && docker cp {0:}:/home/grid_result.json grid_result.json\".format(IMAGENAME, DOCKERIMAGE, PATHTOHOSTFILE_ANALYSISGRID, PATHTOHOSTFILE_SKYMATRIX, PATHTOHOSTFILE_SURFACES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
