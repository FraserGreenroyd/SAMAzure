{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global keys\n",
    "\n",
    "_BATCH_ACCOUNT_NAME = \"climatebasedbatch\"\n",
    "_BATCH_ACCOUNT_KEY = \"W94ukoxG2neFkk6teOVZ3IQ8IQjmPJqPcFq48I9lLzCrPEQSRFS/+euaUEkkSyPoulUgnx5IEZxztA9574Hluw==\"\n",
    "_BATCH_ACCOUNT_URL = \"https://climatebasedbatch.westeurope.batch.azure.com\"\n",
    "\n",
    "_STORAGE_ACCOUNT_NAME = \"radfiles\"\n",
    "_STORAGE_ACCOUNT_KEY = \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "\n",
    "_POOL_ID = \"testpool\"\n",
    "_MIN_POOL_NODE = 1\n",
    "_MAX_POOL_NODE = 100\n",
    "\n",
    "_POOL_VM_SIZE = 'BASIC_A1'\n",
    "_NODE_OS_PUBLISHER = 'Canonical'\n",
    "_NODE_OS_OFFER = 'UbuntuServer'\n",
    "_NODE_OS_SKU = '16'\n",
    "\n",
    "_JOB_NAME = \"0000000-testjob-3513\"\n",
    "\n",
    "_JOB_DIRECTORY = \"./radfiles\"\n",
    "\n",
    "_COPY_TO_BLOB_LOCAL = \"./docker/copy_to_blob.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.models as batchmodels\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob client generated:  <azure.storage.blob.blockblobservice.BlockBlobService object at 0x10d202f28>\n",
      "Credentials generated:  <azure.batch.batch_auth.SharedKeyCredentials object at 0x10d1526d8>\n",
      "Batch client generated: <azure.batch.batch_service_client.BatchServiceClient object at 0x10d2190f0>\n"
     ]
    }
   ],
   "source": [
    "# Create the blob client - for use in obtaining referecnes to the blob stroage containers and uploading files to those containers\n",
    "blob_client = azureblob.BlockBlobService(account_name=_STORAGE_ACCOUNT_NAME, account_key=_STORAGE_ACCOUNT_KEY)\n",
    "print(\"Blob client generated:  {0:}\".format(blob_client))\n",
    "\n",
    "# Generate shard key credentials enabling transaction with the batch client\n",
    "credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME, _BATCH_ACCOUNT_KEY)\n",
    "print(\"Credentials generated:  {0:}\".format(credentials))\n",
    "\n",
    "# Create the batch client - for use in transacting with batch nodes\n",
    "batch_client = batch.BatchServiceClient(credentials, base_url=_BATCH_ACCOUNT_URL)\n",
    "print(\"Batch client generated: {0:}\".format(batch_client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KILL ALL PROCESSES! - TIDY UP AND NEVER LOOK BACK!\n",
    "\n",
    "# containers\n",
    "# blob_client.delete_container(_JOB_NAME)\n",
    "# pools\n",
    "batch_client.pool.delete(_POOL_ID)\n",
    "# jobs\n",
    "# for _JOB_ID in jobs:\n",
    "#     batch_client.job.delete(_JOB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container created: [0000000-testjob-3513]\n"
     ]
    }
   ],
   "source": [
    "# Create container for job file storage\n",
    "blob_client.create_container(_JOB_NAME, fail_on_exist=False)\n",
    "print(\"Container created: [{0:}]\".format(_JOB_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file sky_mtx.json to container [0000000-testjob-3513/sky_mtx.json]\n",
      "Uploading file surfaces.json to container [0000000-testjob-3513/surfaces.json]\n",
      "Uploading file copy_to_blob.py to container [0000000-testjob-3513/copy_to_blob.py]\n",
      "Uploading file zone4.json to container [0000000-testjob-3513/zone4.json]\n",
      "Uploading file zone3.json to container [0000000-testjob-3513/zone3.json]\n",
      "Uploading file zone2.json to container [0000000-testjob-3513/zone2.json]\n",
      "Uploading file zone1.json to container [0000000-testjob-3513/zone1.json]\n"
     ]
    }
   ],
   "source": [
    "# Upload sky matrix and surfaces files into job container\n",
    "\n",
    "def directory_files(directory):\n",
    "    \"\"\"\n",
    "    Generates a list of the files within a directory\n",
    "    :param str directory: A path to a directory.\n",
    "    :return list: List of files\n",
    "    \"\"\"\n",
    "    return [item for sublist in [[os.path.join(path, name) for name in files] for path, subdirs, files in os.walk(directory)] for item in sublist]\n",
    "\n",
    "def upload_file_to_container(block_blob_client, container_name, file_path):\n",
    "    \"\"\"\n",
    "    Uploads a local file to an Azure Blob storage container.\n",
    "    :param block_blob_client: An Azure blockblobservice client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param str file_path: The local path to the file.\n",
    "    :rtype: `azure.batch.models.ResourceFile`\n",
    "    :return: A ResourceFile initialized with a SAS URL appropriate for Batch tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    blob_name = os.path.basename(file_path)\n",
    "    print('Uploading file {0:} to container [{1:}/{0:}]'.format(blob_name, container_name))\n",
    "    block_blob_client.create_blob_from_path(container_name, blob_name, file_path)\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(container_name, blob_name, permission=azureblob.BlobPermissions.READ, expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24))\n",
    "    sas_url = block_blob_client.make_blob_url(container_name, blob_name, sas_token=sas_token)\n",
    "    \n",
    "    return batchmodels.ResourceFile(file_path=blob_name, blob_source=sas_url)\n",
    "\n",
    "# Upload the files common to all simulations (Sky Matrix, Context Surfaces and _APP_FILEs)\n",
    "_SKY_MTX = upload_file_to_container(blob_client, _JOB_NAME, os.path.join(_JOB_DIRECTORY, \"sky_mtx.json\"))\n",
    "_SURFACES = upload_file_to_container(blob_client, _JOB_NAME, os.path.join(_JOB_DIRECTORY, \"surfaces.json\"))\n",
    "_COPY_TO_BLOB = upload_file_to_container(blob_client, _JOB_NAME, _COPY_TO_BLOB_LOCAL)\n",
    "\n",
    "# Upload the files unique to each simulation (individual analysis grids)\n",
    "_ANALYSIS_GRIDS = []\n",
    "for _file in directory_files(os.path.join(_JOB_DIRECTORY, \"AnalysisGrids\")):\n",
    "    _ANALYSIS_GRIDS.append(upload_file_to_container(blob_client, _JOB_NAME, _file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sr=c&sp=w&sig=8NsHFKdc3Ly7HTEKpcW0X1Se3Slo5vY3HZRZNN%2BXtN8%3D&sv=2018-03-28&se=2018-08-19T21%3A54%3A13Z\n"
     ]
    }
   ],
   "source": [
    "# Get shared access signature providing write access to the container\n",
    "def get_container_sas_token(block_blob_client, container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the\n",
    "    container.\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "\n",
    "    container_sas_token = block_blob_client.generate_container_shared_access_signature(\n",
    "        container_name,\n",
    "        permission=blob_permissions,\n",
    "        expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=24))\n",
    "\n",
    "    return container_sas_token\n",
    "\n",
    "output_container_sas_token = get_container_sas_token(blob_client, _JOB_NAME, azureblob.BlobPermissions.WRITE)\n",
    "print(output_container_sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool [testpool]...\n",
      "Pool created ...\n"
     ]
    }
   ],
   "source": [
    "# Create the pool containing the compute nodes executing the tasks\n",
    "def select_latest_verified_vm_image_with_node_agent_sku(batch_client, publisher, offer, sku_starts_with):\n",
    "    \"\"\"Select the latest verified image that Azure Batch supports given a publisher, offer and sku (starts with filter).\n",
    "    :param batch_client: The batch client to use.\n",
    "    :type batch_client: `batchserviceclient.BatchServiceClient`\n",
    "    :param str publisher: vm image publisher\n",
    "    :param str offer: vm image offer\n",
    "    :param str sku_starts_with: vm sku starts with filter\n",
    "    :rtype: tuple\n",
    "    :return: (node agent sku id to use, vm image ref to use)\n",
    "    \"\"\"\n",
    "    # get verified vm image list and node agent sku ids from service\n",
    "    node_agent_skus = batch_client.account.list_node_agent_skus()\n",
    "    # pick the latest supported sku\n",
    "    skus_to_use = [\n",
    "        (sku, image_ref) for sku in node_agent_skus for image_ref in sorted(\n",
    "            sku.verified_image_references, key=lambda item: item.sku)\n",
    "        if image_ref.publisher.lower() == publisher.lower() and\n",
    "        image_ref.offer.lower() == offer.lower() and\n",
    "        image_ref.sku.startswith(sku_starts_with)\n",
    "    ]\n",
    "    # skus are listed in reverse order, pick first for latest\n",
    "    sku_to_use, image_ref_to_use = skus_to_use[0]\n",
    "    return (sku_to_use.id, image_ref_to_use)\n",
    "\n",
    "def wrap_commands_in_shell(ostype, commands):\n",
    "    \"\"\"Wrap commands in a shell\n",
    "    :param list commands: list of commands to wrap\n",
    "    :param str ostype: OS type, linux or windows\n",
    "    :rtype: str\n",
    "    :return: a shell wrapping commands\n",
    "    \"\"\"\n",
    "    if ostype.lower() == \"linux\":\n",
    "        return \"/bin/bash -c \\\"set -e; set -o pipefail; {}; wait\\\"\".format(\";\".join(commands))\n",
    "    elif ostype.lower() == \"windows\":\n",
    "        return \"cmd.exe /c {}\".format(\"&\".join(commands))\n",
    "    else:\n",
    "        raise ValueError(\"unknown ostype: {}\".format(ostype))\n",
    "        \n",
    "def print_batch_exception(batch_exception):\n",
    "    \"\"\"\n",
    "    Prints the contents of the specified Batch exception.\n",
    "    :param batch_exception:\n",
    "    \"\"\"\n",
    "    print('-------------------------------------------')\n",
    "    print('Exception encountered:')\n",
    "    if (batch_exception.error and batch_exception.error.message and\n",
    "            batch_exception.error.message.value):\n",
    "        print(batch_exception.error.message.value)\n",
    "        if batch_exception.error.values:\n",
    "            print()\n",
    "            for mesg in batch_exception.error.values:\n",
    "                print('{}:\\t{}'.format(mesg.key, mesg.value))\n",
    "    print('-------------------------------------------')\n",
    "\n",
    "def create_pool(batch_service_client, pool_id, resource_files, publisher, offer, sku, node_count):\n",
    "    \"\"\"\n",
    "    Creates a pool of compute nodes with the specified OS settings.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str pool_id: An ID for the new pool.\n",
    "    :param list resource_files: A collection of resource files for the pool's start task.\n",
    "    :param str publisher: Marketplace image publisher\n",
    "    :param str offer: Marketplace image offer\n",
    "    :param str sku: Marketplace image sku\n",
    "    \"\"\"\n",
    "    print('Creating pool [{}]...'.format(pool_id))\n",
    "\n",
    "    # Specify the commands for the pool's start task to be run on each node as it joins the pool.\n",
    "    task_commands = [\n",
    "        # Install pip\n",
    "        \"curl -fSsL https://bootstrap.pypa.io/get-pip.py | python\",\n",
    "        # Install the azure-storage module so that the task script can access Azure Blob storage\n",
    "        \"pip install azure-storage==0.32.0\",\n",
    "        # Install docker\n",
    "        \"sudo apt-get install docker -y && sudo apt-get install docker.io -y\",\n",
    "        # Pull RadHoneyWhale from docker hub\n",
    "        \"sudo docker pull tgerrish/bhrad\"]\n",
    "\n",
    "    # Get the node agent SKU and image reference for the virtual machine configuration.\n",
    "    sku_to_use, image_ref_to_use = select_latest_verified_vm_image_with_node_agent_sku(batch_service_client, publisher, offer, sku)\n",
    "    user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "    new_pool = batch.models.PoolAddParameter(\n",
    "        id=pool_id,\n",
    "        virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "            image_reference=image_ref_to_use,\n",
    "            node_agent_sku_id=sku_to_use),\n",
    "        vm_size=_POOL_VM_SIZE,\n",
    "        resize_timeout=datetime.timedelta(minutes=15),\n",
    "        target_dedicated_nodes=_POOL_NODE_COUNT,\n",
    "        start_task=batch.models.StartTask(\n",
    "            command_line=wrap_commands_in_shell(\n",
    "                \"linux\",\n",
    "                task_commands),\n",
    "            user_identity=batchmodels.UserIdentity(auto_user=user),\n",
    "            wait_for_success=True,\n",
    "            resource_files=resource_files),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        batch_service_client.pool.add(new_pool)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "_POOL_NODE_COUNT = len(_ANALYSIS_GRIDS)\n",
    "\n",
    "pool = create_pool(batch_client, _POOL_ID, [_SKY_MTX, _SURFACES, _COPY_TO_BLOB], _NODE_OS_PUBLISHER, _NODE_OS_OFFER, _NODE_OS_SKU, _POOL_NODE_COUNT)\n",
    "\n",
    "print(\"Pool created ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-390c58007a07>, line 87)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-390c58007a07>\"\u001b[0;36m, line \u001b[0;32m87\u001b[0m\n\u001b[0;31m    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\".format(timeout), end='')\u001b[0m\n\u001b[0m                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Add jobs to the pool\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def create_job(batch_service_client, job_id, pool_id):\n",
    "    \"\"\"\n",
    "    Creates a job with the specified ID, associated with the specified pool.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID for the job.\n",
    "    :param str pool_id: The ID for the pool.\n",
    "    \"\"\"\n",
    "    print('Creating job [{0:}]...'.format(job_id))\n",
    "\n",
    "    job = batch.models.JobAddParameter(job_id, batch.models.PoolInformation(pool_id=pool_id))\n",
    "\n",
    "    try:\n",
    "        batch_service_client.job.add(job)\n",
    "    except batchmodels.batch_error.BatchErrorException as err:\n",
    "        print_batch_exception(err)\n",
    "        raise\n",
    "\n",
    "def add_tasks(batch_service_client, job_id, input_files,\n",
    "              input_file_names, output_container_name, output_container_sas_token, other_input_files):\n",
    "    \"\"\"\n",
    "    Adds a task for each input file in the collection to the specified job.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID of the job to which to add the tasks.\n",
    "    :param list input_files: A collection of input files. One task will be\n",
    "     created for each input file.\n",
    "    :param output_container_name: The ID of an Azure Blob storage container to\n",
    "    which the tasks will upload their results.\n",
    "    :param output_container_sas_token: A SAS token granting write access to\n",
    "    the specified Azure Blob storage container.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Adding {} tasks to job [{}]...'.format(len(input_files), job_id))\n",
    "\n",
    "    user = batchmodels.UserIdentity(\n",
    "    auto_user=batchmodels.AutoUserSpecification(\n",
    "        elevation_level=batchmodels.ElevationLevel.admin,\n",
    "        scope=batchmodels.AutoUserScope.task))\n",
    "\n",
    "    tasks = list()\n",
    "\n",
    "    for idx, input_file in enumerate(input_files):\n",
    "        filename = input_file_names[idx]\n",
    "\n",
    "        command = [\"sudo bash\",\n",
    "                   \"sudo docker run -v $AZ_BATCH_TASK_WORKING_DIR:$HOME/home -a stdout tgerrish/radtemp:latest python RunHoneybeeRadiance.py -p $HOME/home/zone1.json -sm $HOME/home/sky_mtx.json -s $HOME/home/surfaces.json\",\n",
    "                   \"python $AZ_BATCH_NODE_SHARED_DIR/copy_to_blob.py --filepath {0:} --blobname {1:} --storageaccount {2:} --storagecontainer {3:} --sastoken '{4:}'\".format(\n",
    "                       \"$AZ_BATCH_NODE_SHARED_DIR/{0:}\".format(filename.replace(\".json\", \"_result.json\")),\n",
    "                        filename.replace(\".json\", \"_result.json\"),\n",
    "                       _STORAGE_ACCOUNT_NAME,\n",
    "                       output_container_name,\n",
    "                       output_container_sas_token\n",
    "                   )]\n",
    "\n",
    "        tasks.append(batch.models.TaskAddParameter(\n",
    "                'Task_{}'.format(idx,filename),\n",
    "                common.helpers.wrap_commands_in_shell('linux', command),\n",
    "                resource_files=[input_file],\n",
    "                user_identity=user\n",
    "                )\n",
    "        )\n",
    "    batch_service_client.task.add_collection(job_id, tasks)\n",
    "\n",
    "def wait_for_tasks_to_complete(batch_service_client, job_id, timeout):\n",
    "    \"\"\"\n",
    "    Returns when all tasks in the specified job reach the Completed state.\n",
    "\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The id of the job whose tasks should be to monitored.\n",
    "    :param timedelta timeout: The duration to wait for task completion. If all\n",
    "    tasks in the specified job do not reach Completed state within this time\n",
    "    period, an exception will be raised.\n",
    "    \"\"\"\n",
    "    timeout_expiration = datetime.datetime.now() + timeout\n",
    "\n",
    "    print(\"Monitoring all tasks for 'Completed' state, timeout in {}...\".format(timeout), end='')\n",
    "\n",
    "    while datetime.datetime.now() < timeout_expiration:\n",
    "        print('.', end='')\n",
    "        sys.stdout.flush()\n",
    "        tasks = [batch_service_client.task.list(job) for job in job_id]\n",
    "\n",
    "        tasks = [item for sublist in tasks for item in sublist]\n",
    "\n",
    "        incomplete_tasks = [task for task in tasks if\n",
    "                            task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            print()\n",
    "            return True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    print()\n",
    "    raise RuntimeError(\"ERROR: Tasks did not reach 'Completed' state within \"\n",
    "                       \"timeout period of \" + str(timeout))\n",
    "\n",
    "jobs = list()\n",
    "_JOB_ID = \"radjob\"\n",
    "\n",
    "for i, (files, names) in enumerate(zip(list(chunks(grid_files, 100)),list(chunks(grid_file_names, 100)))):\n",
    "    job_id = _JOB_ID + \"_\" + str(i)\n",
    "    jobs.append(job_id)\n",
    "\n",
    "    # Create the job that will run the tasks.\n",
    "    create_job(batch_client, job_id, _POOL_ID)\n",
    "\n",
    "    # Add the tasks to the job. We need to supply a container shared access\n",
    "    # signature (SAS) token for the tasks so that they can upload their output\n",
    "    # to Azure Storage.\n",
    "\n",
    "    add_tasks(batch_client,\n",
    "              job_id,\n",
    "              files,\n",
    "              names,\n",
    "              _JOB_NAME,\n",
    "              output_container_sas_token)\n",
    "\n",
    "\n",
    "# Pause execution until tasks reach Completed state.\n",
    "wait_for_tasks_to_complete(batch_client,\n",
    "                           jobs,\n",
    "                           datetime.timedelta(hours=24))\n",
    "\n",
    "print(\"  Success! All tasks reached the 'Completed' state within the \"\n",
    "      \"specified timeout period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file_paths\n",
    "#grid_file_names\n",
    "grid_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = batchmodels.UserIdentity(auto_user=batchmodels.AutoUserSpecification(elevation_level=batchmodels.ElevationLevel.admin, scope=batchmodels.AutoUserScope.task))\n",
    "\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     # user = batchmodels.UserIdentity(auto_user=batchmodels.AutoUserSpecification(elevation_level=batchmodels.ElevationLevel.admin, scope=batchmodels.AutoUserScope.task))\n",
    "\n",
    "#     tasks = []\n",
    "\n",
    "#     for idx, input_file in enumerate(input_files):\n",
    "#         filename = input_file_names[idx].split(\".\")[0]\n",
    "\n",
    "#         command = [\n",
    "#             \"sudo bash\",\n",
    "#             \"sudo docker run -it -d --name temp tgerrish/bhrad:latest\",\n",
    "#             \"sudo docker cp $AZ_BATCH_TASK_WORKING_DIR/analysisGrid.json temp:/grid.json\",\n",
    "#             \"sudo docker cp $AZ_BATCH_TASK_WORKING_DIR/sky_mtx.json temp:/sky_mtx.json\",\n",
    "#             \"sudo docker cp $AZ_BATCH_TASK_WORKING_DIR/surfaces.json temp:/surfaces.json\"\n",
    "#             \"sudo docker exec temp python RunHoneybeeRadiance.py -p ./grid.json -sm ./sky_mtx.json -s ./surfaces.json\"\n",
    "#             \"sudo docker cp temp:/grid_result.json grid_result.json\"\n",
    "#             --filepath RunHoneybeeRadiance.py --blobname \"runhbrad.py\" --storageaccount \"radfiles\" --storagecontainer \"0000000-jobname-3513\" --sastoken \"aRRVzOkO/kwS35CIwNVIa18aGoMfZD5D3yAy3GlorkkU2G+9q5rAscXoC21IIylJZerBefwMgxYYF3qzquALrw==\"\n",
    "            \n",
    "#             'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job:z antoinedao/radhoneywhale unzip /usr/job/{} -d /usr/job/'.format(input_file_names[idx]),\n",
    "#                     'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale python3 /usr/convertToBash.py '\n",
    "#                     '--filepath /usr/job',\n",
    "#                     'sudo chmod +x $AZ_BATCH_TASK_WORKING_DIR/gridbased_annual/commands.sh',\n",
    "#                     'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale /usr/job/gridbased_annual/commands.sh',\n",
    "#                     'sudo docker run --rm -v $AZ_BATCH_TASK_WORKING_DIR:/usr/job antoinedao/radhoneywhale zip -r /usr/job/out_{} /usr/job/'.format(filename + \".zip\"),\n",
    "#                     'python $AZ_BATCH_NODE_SHARED_DIR/copyToBlob.py '\n",
    "#                     '--filepath $AZ_BATCH_TASK_WORKING_DIR/{} --filename {} --storageaccount {} '\n",
    "#                     '--storagecontainer {} --sastoken \"{}\"'.format(\n",
    "#                      'out_'+filename + \".zip\",\n",
    "#                      'out_'+filename + \".zip\",\n",
    "#                      _STORAGE_ACCOUNT_NAME,\n",
    "#                      output_container_name,\n",
    "#                      output_container_sas_token)]\n",
    "\n",
    "#         tasks.append(batch.models.TaskAddParameter(\n",
    "#                 'Task_{}'.format(idx,filename),\n",
    "#                 common.helpers.wrap_commands_in_shell('linux', command),\n",
    "#                 resource_files=[input_file],\n",
    "#                 user_identity=user\n",
    "#                 )\n",
    "#         )\n",
    "\n",
    "#     batch_service_client.task.add_collection(job_id, tasks)\n",
    "\n",
    "batch_service_client = batch_client\n",
    "job_id = _JOB_ID + \"_0\"\n",
    "input_files\n",
    "input_file_names\n",
    "output_container_name\n",
    "output_container_sas_token\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMANDS FOR THE BATCH NODE\n",
    "\n",
    "DOCKERIMAGE = \"bhrad:latest\"\n",
    "IMAGENAME = \"xyz\"\n",
    "PATHTOHOSTFILE_ANALYSISGRID = \"C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/AnalysisGrids/zone1.json\"\n",
    "PATHTOHOSTFILE_SKYMATRIX = \"C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/sky_mtx.json\"\n",
    "PATHTOHOSTFILE_SURFACES = \"C:/Users/tgerrish/Documents/GitHub/SAMAzure/TestFiles/Azure/radfiles/surfaces.json\"\n",
    "\n",
    "# Start docker container\n",
    "print(\"docker run --name {0:} {1:}\".format(IMAGENAME, DOCKERIMAGE))\n",
    "\n",
    "# Copy files from host to container\n",
    "print(\"docker cp {0:} {1:}:/grid.json\".format(PATHTOHOSTFILE_ANALYSISGRID, IMAGENAME))\n",
    "print(\"docker cp {0:} {1:}:/sky_mtx.json\".format(PATHTOHOSTFILE_SKYMATRIX, IMAGENAME))\n",
    "print(\"docker cp {0:} {1:}:/surfaces.json\".format(PATHTOHOSTFILE_SURFACES, IMAGENAME))\n",
    "\n",
    "# Run Radiance command for daylight metrics processing\n",
    "print(\"docker exec {0:} python RunHoneybeeRadiance.py -p ./grid.json -sm ./sky_mtx.json -s ./surfaces.json -o ./home\".format(IMAGENAME))\n",
    "\n",
    "# Copy result file back to host\n",
    "print(\"docker cp {0:}:/home/grid_result.json grid_result.json\".format(IMAGENAME))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"docker run --name {0:} {1:} \\\\\\n&& docker cp {2:} {0:}:/home/grid.json \\\\\\n&& docker cp {3:} {0:}:/home/sky_mtx.json \\\\\\n&& docker cp {4:} {0:}:/home/surfaces.json \\\\\\n&& docker exec {0:} python _RunHoneybeeRadiance.py -p ./home/grid.json -sm ./home/sky_mtx.json -s ./home/surfaces.json -o ./home \\\\\\n&& docker cp {0:}:/home/grid_result.json grid_result.json\".format(IMAGENAME, DOCKERIMAGE, PATHTOHOSTFILE_ANALYSISGRID, PATHTOHOSTFILE_SKYMATRIX, PATHTOHOSTFILE_SURFACES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"docker run --name {0:} {1:} && sleep 2 && docker cp {2:} {0:}:/home/grid.json && docker cp {3:} {0:}:/home/sky_mtx.json && docker cp {4:} {0:}:/home/surfaces.json && docker exec {0:} python _RunHoneybeeRadiance.py -p ./home/grid.json -sm ./home/sky_mtx.json -s ./home/surfaces.json -o ./home && docker cp {0:}:/home/grid_result.json grid_result.json\".format(IMAGENAME, DOCKERIMAGE, PATHTOHOSTFILE_ANALYSISGRID, PATHTOHOSTFILE_SKYMATRIX, PATHTOHOSTFILE_SURFACES))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
